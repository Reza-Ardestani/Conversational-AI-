{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NyJCp0fEb2GT"
      },
      "source": [
        "# 1) Introduction\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0ouX6hOJ17W"
      },
      "source": [
        "<h2> Covered in the previous notebook </h2>\n",
        "<br>\n",
        "<br>\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3zWyhhFpcC3v"
      },
      "source": [
        "# 2) Project Management"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urKeduhMKFSv"
      },
      "source": [
        "<h2> Covered in the previous notebook </h2>\n",
        "<br>\n",
        "<br>\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1EoWuvKEcFax"
      },
      "source": [
        "# 3) Env Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "2o-G9p6QcLN7"
      },
      "outputs": [],
      "source": [
        "!pip install langchain==0.2.3 langchain_experimental umap-learn scikit-learn langchain_community tiktoken langchain-openai langchainhub chromadb langchain-anthropic wikipedia\n",
        "!pip install beautifulsoup4 lxml # for scraping and parsing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z55tnrvHyGWB"
      },
      "outputs": [],
      "source": [
        "!pip install -q streamlit\n",
        "!npm install localtunnel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UZ6Bj0rjdrF-",
        "outputId": "abd172ec-8aff-4449-ed7c-ce9c2d9e16f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing requirements_Inventures_RAG_with_UI.txt\n"
          ]
        }
      ],
      "source": [
        "%%writefile requirements_Inventures_RAG_with_UI.txt\n",
        "# pip freeze on Google Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "45e2bPB3doLJ"
      },
      "outputs": [],
      "source": [
        "!pip freeze >> requirements_Inventures_RAG.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "rUk5VkHqgd6r"
      },
      "outputs": [],
      "source": [
        "%%writefile mylibs.py\n",
        "# Importing libraries\n",
        "import os\n",
        "import pickle\n",
        "import json # saving sessions data in a json file\n",
        "from google.colab import userdata # for importing env variable on Colab\n",
        "import matplotlib.pyplot as plt\n",
        "import tiktoken # for counting the # of tokens in each document\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import umap # for dimensionality reduction\n",
        "from sklearn.mixture import GaussianMixture # for clustering\n",
        "from typing import Dict, List, Optional, Tuple # for type hinting\n",
        "\n",
        "from bs4 import BeautifulSoup # for scraping and parsing\n",
        "\n",
        "# Wikiloader in case we want to retrieve data from Wikipedia as a trainset\n",
        "from langchain_community.document_loaders import WikipediaLoader\n",
        "# converting doc_text files into langchain document objects\n",
        "from langchain.docstore.document import Document\n",
        "\n",
        "from langchain_openai import OpenAIEmbeddings # For text-to-vector\n",
        "from langchain_openai import ChatOpenAI # For API call to LLM\n",
        "from langchain_anthropic import ChatAnthropic # Alternative to OpenAI LLM\n",
        "\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "RANDOM_SEED = 307  # Fixed seed for reproducibility of clustering\n",
        "\n",
        "from langchain_community.vectorstores import Chroma #In-memory VectorDB\n",
        "\n",
        "from langchain import hub # Langchain Hub for RAG prompt templates\n",
        "# placeholder used in chain\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "import streamlit as st\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "_OjeMejMJsDd"
      },
      "outputs": [],
      "source": [
        "# Args and global variables\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\")\n",
        "os.environ[\"ANTHROPIC_API_KEY\"] = 'anthropic-api-key'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0g9XvUBcMRI"
      },
      "source": [
        "# 4) Data Handeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XArnMsdopGNf"
      },
      "source": [
        "## 4.1 Scraping and parsing the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "8yBGwuPmcQRG"
      },
      "outputs": [],
      "source": [
        "def parse_sessions_html(html_file_add):\n",
        "  # Read the HTML file\n",
        "  with open(html_file_add, 'r', encoding='utf-8') as file:\n",
        "    html_content = file.read()\n",
        "\n",
        "  # Parse the HTML content using BeautifulSoup\n",
        "  soup = BeautifulSoup(html_content, 'lxml')\n",
        "\n",
        "  # Extract session details\n",
        "  sessions = []\n",
        "  session_divs = soup.find_all('div', attrs={\"x-html\": \"session.card_template\"})\n",
        "\n",
        "  for session_div in session_divs:\n",
        "    session = {}\n",
        "\n",
        "    # Extract session category\n",
        "    category_tag = session_div.find('span', class_=\"block w-fit bg-green py-2 -mt-1 md:ml-0 px-6 lg:pr-8 text-xs font-moderna rounded-br-full\")\n",
        "    session['category'] = category_tag.text.strip() if category_tag else None\n",
        "\n",
        "    # Extract session title\n",
        "    title_tag = session_div.find('h2', class_=\"font-moderna uppercase text-base xl:text-lg text-purple\")\n",
        "    session['title'] = title_tag.text.strip() if title_tag else None\n",
        "\n",
        "    # Extract session description\n",
        "    description_tag = session_div.find('div', class_=\"cursor-pointer text-base line-clamp-2\")\n",
        "    session['description'] = description_tag.text.strip() if description_tag else None\n",
        "\n",
        "    # Extract speakers\n",
        "    speakers = []\n",
        "    speaker_section = session_div.find('div', class_=\"md:border-t border-slate-200 md:pt-4\")\n",
        "    if speaker_section:\n",
        "      speaker_buttons = speaker_section.find_all('button', class_=\"group relative flex gap-4 justify-start items-center !no-underline\")\n",
        "      for speaker_button in speaker_buttons:\n",
        "        speaker = {}\n",
        "\n",
        "        # Extract speaker name\n",
        "        name_tag = speaker_button.find('h4', class_=\"text-sm font-moderna\")\n",
        "        speaker['name'] = name_tag.text.strip() if name_tag else None\n",
        "\n",
        "        # Extract speaker position\n",
        "        position_tag = speaker_button.find('p', class_=\"override text-sm leading-tight text-slate-500\")\n",
        "        speaker['position'] = position_tag.text.strip() if position_tag else None\n",
        "\n",
        "        speakers.append(speaker)\n",
        "\n",
        "    session['speakers'] = speakers\n",
        "    sessions.append(session)\n",
        "\n",
        "  sessions.pop(0) # The first session is empty\n",
        "  # Convert the sessions list to a JSON string\n",
        "  json_string = json.dumps(sessions)\n",
        "  # Write the JSON string to a file\n",
        "  with open('sessions.json', 'w') as f:\n",
        "    f.write(json_string)\n",
        "\n",
        "  return sessions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "FUSIRG0Gq_oi"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUzl7VO7rA-o"
      },
      "source": [
        "## 4.2) Processing data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "IYwTYVjwthm5"
      },
      "outputs": [],
      "source": [
        "def save_sessions_to_text_files(sessions):\n",
        "    \"\"\"Save sessions to a separate text files.\"\"\"\n",
        "    for i, session in enumerate(sessions, start=1):\n",
        "      category = session['category']\n",
        "      title = session['title']\n",
        "      description = session['description']\n",
        "      speakers = session['speakers']\n",
        "      content = f\"A session under the category of : {category} which is titled as: {title} is held at Inventures. The description of this session is: {description}. \\n \"\n",
        "\n",
        "      if speakers:\n",
        "        content += \"The speakers of this session are: \\n\"\n",
        "      for speaker in speakers:\n",
        "        name = speaker['name']\n",
        "        position = speaker['position']\n",
        "        content += f\"  - Name: {name} who has a position: {position}\\n\"\n",
        "\n",
        "      content += \"\\n\"\n",
        "\n",
        "      with open(f\"./sessions_txt/{i}.txt\", \"a\", encoding='utf-8') as file:\n",
        "        file.write(content)\n",
        "        file.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7tNhbFfcRT8"
      },
      "source": [
        "# 5) Modeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "oSaisYShcVTu"
      },
      "outputs": [],
      "source": [
        "# Reference for this cell:\n",
        "# https://github.com/langchain-ai/langchain/blob/master/cookbook/\n",
        "\n",
        "### --- Code from citations referenced above (added comments and docstrings) --- ###\n",
        "\n",
        "\n",
        "def global_cluster_embeddings(\n",
        "    embeddings: np.ndarray,\n",
        "    dim: int,\n",
        "    n_neighbors: Optional[int] = None,\n",
        "    metric: str = \"cosine\",\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Perform global dimensionality reduction on the embeddings using UMAP.\n",
        "\n",
        "    Parameters:\n",
        "    - embeddings: The input embeddings as a numpy array.\n",
        "    - dim: The target dimensionality for the reduced space.\n",
        "    - n_neighbors: Optional; the number of neighbors to consider for each point.\n",
        "                   If not provided, it defaults to the square root of the number of embeddings.\n",
        "    - metric: The distance metric to use for UMAP.\n",
        "\n",
        "    Returns:\n",
        "    - A numpy array of the embeddings reduced to the specified dimensionality.\n",
        "    \"\"\"\n",
        "    if n_neighbors is None:\n",
        "        n_neighbors = int((len(embeddings) - 1) ** 0.5)\n",
        "    return umap.UMAP(\n",
        "        n_neighbors=n_neighbors, n_components=dim, metric=metric\n",
        "    ).fit_transform(embeddings)\n",
        "\n",
        "\n",
        "def local_cluster_embeddings(\n",
        "    embeddings: np.ndarray, dim: int, num_neighbors: int = 10, metric: str = \"cosine\"\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Perform local dimensionality reduction on the embeddings using UMAP, typically after global clustering.\n",
        "\n",
        "    Parameters:\n",
        "    - embeddings: The input embeddings as a numpy array.\n",
        "    - dim: The target dimensionality for the reduced space.\n",
        "    - num_neighbors: The number of neighbors to consider for each point.\n",
        "    - metric: The distance metric to use for UMAP.\n",
        "\n",
        "    Returns:\n",
        "    - A numpy array of the embeddings reduced to the specified dimensionality.\n",
        "    \"\"\"\n",
        "    return umap.UMAP(\n",
        "        n_neighbors=num_neighbors, n_components=dim, metric=metric\n",
        "    ).fit_transform(embeddings)\n",
        "\n",
        "\n",
        "def get_optimal_clusters(\n",
        "    embeddings: np.ndarray, max_clusters: int = 50, random_state: int = RANDOM_SEED\n",
        ") -> int:\n",
        "    \"\"\"\n",
        "    Determine the optimal number of clusters using the Bayesian Information Criterion (BIC) with a Gaussian Mixture Model.\n",
        "\n",
        "    Parameters:\n",
        "    - embeddings: The input embeddings as a numpy array.\n",
        "    - max_clusters: The maximum number of clusters to consider.\n",
        "    - random_state: Seed for reproducibility.\n",
        "\n",
        "    Returns:\n",
        "    - An integer representing the optimal number of clusters found.\n",
        "    \"\"\"\n",
        "    max_clusters = min(max_clusters, len(embeddings))\n",
        "    n_clusters = np.arange(1, max_clusters)\n",
        "    bics = []\n",
        "    for n in n_clusters:\n",
        "        gm = GaussianMixture(n_components=n, random_state=random_state)\n",
        "        gm.fit(embeddings)\n",
        "        bics.append(gm.bic(embeddings))\n",
        "    return n_clusters[np.argmin(bics)]\n",
        "\n",
        "\n",
        "def GMM_cluster(embeddings: np.ndarray, threshold: float, random_state: int = 0):\n",
        "    \"\"\"\n",
        "    Cluster embeddings using a Gaussian Mixture Model (GMM) based on a probability threshold.\n",
        "\n",
        "    Parameters:\n",
        "    - embeddings: The input embeddings as a numpy array.\n",
        "    - threshold: The probability threshold for assigning an embedding to a cluster.\n",
        "    - random_state: Seed for reproducibility.\n",
        "\n",
        "    Returns:\n",
        "    - A tuple containing the cluster labels and the number of clusters determined.\n",
        "    \"\"\"\n",
        "    n_clusters = get_optimal_clusters(embeddings)\n",
        "    gm = GaussianMixture(n_components=n_clusters, random_state=random_state)\n",
        "    gm.fit(embeddings)\n",
        "    probs = gm.predict_proba(embeddings)\n",
        "    labels = [np.where(prob > threshold)[0] for prob in probs]\n",
        "    return labels, n_clusters\n",
        "\n",
        "\n",
        "def perform_clustering(\n",
        "    embeddings: np.ndarray,\n",
        "    dim: int,\n",
        "    threshold: float,\n",
        ") -> List[np.ndarray]:\n",
        "    \"\"\"\n",
        "    Perform clustering on the embeddings by first reducing their dimensionality globally, then clustering\n",
        "    using a Gaussian Mixture Model, and finally performing local clustering within each global cluster.\n",
        "\n",
        "    Parameters:\n",
        "    - embeddings: The input embeddings as a numpy array.\n",
        "    - dim: The target dimensionality for UMAP reduction.\n",
        "    - threshold: The probability threshold for assigning an embedding to a cluster in GMM.\n",
        "\n",
        "    Returns:\n",
        "    - A list of numpy arrays, where each array contains the cluster IDs for each embedding.\n",
        "    \"\"\"\n",
        "    if len(embeddings) <= dim + 1:\n",
        "        # Avoid clustering when there's insufficient data\n",
        "        return [np.array([0]) for _ in range(len(embeddings))]\n",
        "\n",
        "    # Global dimensionality reduction\n",
        "    reduced_embeddings_global = global_cluster_embeddings(embeddings, dim)\n",
        "    # Global clustering\n",
        "    global_clusters, n_global_clusters = GMM_cluster(\n",
        "        reduced_embeddings_global, threshold\n",
        "    )\n",
        "\n",
        "    all_local_clusters = [np.array([]) for _ in range(len(embeddings))]\n",
        "    total_clusters = 0\n",
        "\n",
        "    # Iterate through each global cluster to perform local clustering\n",
        "    for i in range(n_global_clusters):\n",
        "        # Extract embeddings belonging to the current global cluster\n",
        "        global_cluster_embeddings_ = embeddings[\n",
        "            np.array([i in gc for gc in global_clusters])\n",
        "        ]\n",
        "\n",
        "        if len(global_cluster_embeddings_) == 0:\n",
        "            continue\n",
        "        if len(global_cluster_embeddings_) <= dim + 1:\n",
        "            # Handle small clusters with direct assignment\n",
        "            local_clusters = [np.array([0]) for _ in global_cluster_embeddings_]\n",
        "            n_local_clusters = 1\n",
        "        else:\n",
        "            # Local dimensionality reduction and clustering\n",
        "            reduced_embeddings_local = local_cluster_embeddings(\n",
        "                global_cluster_embeddings_, dim\n",
        "            )\n",
        "            local_clusters, n_local_clusters = GMM_cluster(\n",
        "                reduced_embeddings_local, threshold\n",
        "            )\n",
        "\n",
        "        # Assign local cluster IDs, adjusting for total clusters already processed\n",
        "        for j in range(n_local_clusters):\n",
        "            local_cluster_embeddings_ = global_cluster_embeddings_[\n",
        "                np.array([j in lc for lc in local_clusters])\n",
        "            ]\n",
        "            indices = np.where(\n",
        "                (embeddings == local_cluster_embeddings_[:, None]).all(-1)\n",
        "            )[1]\n",
        "            for idx in indices:\n",
        "                all_local_clusters[idx] = np.append(\n",
        "                    all_local_clusters[idx], j + total_clusters\n",
        "                )\n",
        "\n",
        "        total_clusters += n_local_clusters\n",
        "\n",
        "    return all_local_clusters\n",
        "\n",
        "\n",
        "### --- Our code below --- ###\n",
        "\n",
        "\n",
        "def embed(texts):\n",
        "    \"\"\"\n",
        "    Generate embeddings for a list of text documents.\n",
        "\n",
        "    This function assumes the existence of an `embd` object with a method `embed_documents`\n",
        "    that takes a list of texts and returns their embeddings.\n",
        "\n",
        "    Parameters:\n",
        "    - texts: List[str], a list of text documents to be embedded.\n",
        "\n",
        "    Returns:\n",
        "    - numpy.ndarray: An array of embeddings for the given text documents.\n",
        "    \"\"\"\n",
        "    text_embeddings = embd.embed_documents(texts)\n",
        "    text_embeddings_np = np.array(text_embeddings)\n",
        "    return text_embeddings_np\n",
        "\n",
        "\n",
        "def embed_cluster_texts(texts):\n",
        "    \"\"\"\n",
        "    Embeds a list of texts and clusters them, returning a DataFrame with texts, their embeddings, and cluster labels.\n",
        "\n",
        "    This function combines embedding generation and clustering into a single step. It assumes the existence\n",
        "    of a previously defined `perform_clustering` function that performs clustering on the embeddings.\n",
        "\n",
        "    Parameters:\n",
        "    - texts: List[str], a list of text documents to be processed.\n",
        "\n",
        "    Returns:\n",
        "    - pandas.DataFrame: A DataFrame containing the original texts, their embeddings, and the assigned cluster labels.\n",
        "    \"\"\"\n",
        "    text_embeddings_np = embed(texts)  # Generate embeddings\n",
        "    cluster_labels = perform_clustering(\n",
        "        text_embeddings_np, 10, 0.1\n",
        "    )  # Perform clustering on the embeddings\n",
        "    df = pd.DataFrame()  # Initialize a DataFrame to store the results\n",
        "    df[\"text\"] = texts  # Store original texts\n",
        "    df[\"embd\"] = list(text_embeddings_np)  # Store embeddings as a list in the DataFrame\n",
        "    df[\"cluster\"] = cluster_labels  # Store cluster labels\n",
        "    return df\n",
        "\n",
        "\n",
        "def fmt_txt(df: pd.DataFrame) -> str:\n",
        "    \"\"\"\n",
        "    Formats the text documents in a DataFrame into a single string.\n",
        "\n",
        "    Parameters:\n",
        "    - df: DataFrame containing the 'text' column with text documents to format.\n",
        "\n",
        "    Returns:\n",
        "    - A single string where all text documents are joined by a specific delimiter.\n",
        "    \"\"\"\n",
        "    unique_txt = df[\"text\"].tolist()\n",
        "    return \"--- --- \\n --- --- \".join(unique_txt)\n",
        "\n",
        "\n",
        "def embed_cluster_summarize_texts(\n",
        "    texts: List[str], level: int\n",
        ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Embeds, clusters, and summarizes a list of texts. This function first generates embeddings for the texts,\n",
        "    clusters them based on similarity, expands the cluster assignments for easier processing, and then summarizes\n",
        "    the content within each cluster.\n",
        "\n",
        "    Parameters:\n",
        "    - texts: A list of text documents to be processed.\n",
        "    - level: An integer parameter that could define the depth or detail of processing.\n",
        "\n",
        "    Returns:\n",
        "    - Tuple containing two DataFrames:\n",
        "      1. The first DataFrame (`df_clusters`) includes the original texts, their embeddings, and cluster assignments.\n",
        "      2. The second DataFrame (`df_summary`) contains summaries for each cluster, the specified level of detail,\n",
        "         and the cluster identifiers.\n",
        "    \"\"\"\n",
        "\n",
        "    # Embed and cluster the texts, resulting in a DataFrame with 'text', 'embd', and 'cluster' columns\n",
        "    df_clusters = embed_cluster_texts(texts)\n",
        "\n",
        "    # Prepare to expand the DataFrame for easier manipulation of clusters\n",
        "    expanded_list = []\n",
        "\n",
        "    # Expand DataFrame entries to document-cluster pairings for straightforward processing\n",
        "    for index, row in df_clusters.iterrows():\n",
        "        for cluster in row[\"cluster\"]:\n",
        "            expanded_list.append(\n",
        "                {\"text\": row[\"text\"], \"embd\": row[\"embd\"], \"cluster\": cluster}\n",
        "            )\n",
        "\n",
        "    # Create a new DataFrame from the expanded list\n",
        "    expanded_df = pd.DataFrame(expanded_list)\n",
        "\n",
        "    # Retrieve unique cluster identifiers for processing\n",
        "    all_clusters = expanded_df[\"cluster\"].unique()\n",
        "\n",
        "    print(f\"--Generated {len(all_clusters)} clusters--\")\n",
        "\n",
        "    # Summarization\n",
        "    template = \"\"\"\n",
        "    Give a detailed summary of the documentation provided.\n",
        "\n",
        "    Documentation:\n",
        "    {context}\n",
        "    \"\"\"\n",
        "    prompt = ChatPromptTemplate.from_template(template)\n",
        "    chain = prompt | model | StrOutputParser()\n",
        "\n",
        "    # Format text within each cluster for summarization\n",
        "    summaries = []\n",
        "    for i in all_clusters:\n",
        "        df_cluster = expanded_df[expanded_df[\"cluster\"] == i]\n",
        "        formatted_txt = fmt_txt(df_cluster)\n",
        "        summaries.append(chain.invoke({\"context\": formatted_txt}))\n",
        "\n",
        "    # Create a DataFrame to store summaries with their corresponding cluster and level\n",
        "    df_summary = pd.DataFrame(\n",
        "        {\n",
        "            \"summaries\": summaries,\n",
        "            \"level\": [level] * len(summaries),\n",
        "            \"cluster\": list(all_clusters),\n",
        "        }\n",
        "    )\n",
        "\n",
        "    return df_clusters, df_summary\n",
        "\n",
        "\n",
        "def recursive_embed_cluster_summarize(\n",
        "    texts: List[str], level: int = 1, n_levels: int = 3\n",
        ") -> Dict[int, Tuple[pd.DataFrame, pd.DataFrame]]:\n",
        "    \"\"\"\n",
        "    Recursively embeds, clusters, and summarizes texts up to a specified level or until\n",
        "    the number of unique clusters becomes 1, storing the results at each level.\n",
        "\n",
        "    Parameters:\n",
        "    - texts: List[str], texts to be processed.\n",
        "    - level: int, current recursion level (starts at 1).\n",
        "    - n_levels: int, maximum depth of recursion.\n",
        "\n",
        "    Returns:\n",
        "    - Dict[int, Tuple[pd.DataFrame, pd.DataFrame]], a dictionary where keys are the recursion\n",
        "      levels and values are tuples containing the clusters DataFrame and summaries DataFrame at that level.\n",
        "    \"\"\"\n",
        "    results = {}  # Dictionary to store results at each level\n",
        "\n",
        "    # Perform embedding, clustering, and summarization for the current level\n",
        "    df_clusters, df_summary = embed_cluster_summarize_texts(texts, level)\n",
        "\n",
        "    # Store the results of the current level\n",
        "    results[level] = (df_clusters, df_summary)\n",
        "\n",
        "    # Determine if further recursion is possible and meaningful\n",
        "    unique_clusters = df_summary[\"cluster\"].nunique()\n",
        "    if level < n_levels and unique_clusters > 1:\n",
        "        # Use summaries as the input texts for the next level of recursion\n",
        "        new_texts = df_summary[\"summaries\"].tolist()\n",
        "        next_level_results = recursive_embed_cluster_summarize(\n",
        "            new_texts, level + 1, n_levels\n",
        "        )\n",
        "\n",
        "        # Merge the results from the next level into the current results dictionary\n",
        "        results.update(next_level_results)\n",
        "\n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GIOjU0idcXNn"
      },
      "source": [
        "# 6) Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p1GCinrWcW7a"
      },
      "outputs": [],
      "source": [
        "# We're using human evaluation on different aspects of automatic text\n",
        "# summarization and question answering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6jfh0_f8chSX"
      },
      "source": [
        "# 7) Deployment & Maintnance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "DLgt5t5fcv_h"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!unzip sessions_txt.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yLeBUcPY8cVs",
        "outputId": "811ac95f-537d-497c-92c4-df5965357da3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "#initial files\n",
        "import streamlit as st\n",
        "\n",
        "st.title(\"RAG Bot\")\n",
        "with st.chat_message(\"assistant\"):\n",
        "  st.write(\"Hello! Please wait till I spin up 🔄 ...\")\n",
        "\n",
        "if 'initialized' not in st.session_state:\n",
        "  # python does not have block-level scope within conditional\n",
        "  # statements (if, for, while, etc.). Instead, Python uses function-level\n",
        "  # scope. This means that variables defined inside an if statement\n",
        "  # are still accessible outside of that statement, provided they are\n",
        "  # within the same function or global scope. But, streamlit only remembers\n",
        "  # the st.session_state in subsequent runs. That's why we need to store\n",
        "  # rag_chain in state\n",
        "  from mylibs import *\n",
        "  st.session_state.initialized = True\n",
        "  # Initialize chat history\n",
        "  if \"messages\" not in st.session_state:\n",
        "    st.session_state.messages = []\n",
        "\n",
        "  doc_text = [] # loading the txt files into a list of strings\n",
        "  for i in range(1,187):\n",
        "    with open(f'./sessions_txt/{i}.txt', 'r') as file:\n",
        "      data = file.read()\n",
        "      doc_text.append(data)\n",
        "\n",
        "  raw_documents = []\n",
        "  for i in range(len(doc_text)):\n",
        "    raw_documents.append(Document(page_content=doc_text[i], metadata={\"source\": \"local\", 'id':i+1}))\n",
        "\n",
        "  embd = OpenAIEmbeddings()\n",
        "  model = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo\", max_tokens=4096)\n",
        "  leaf_texts = doc_text\n",
        "  # unpickle results.pkl into results\n",
        "  with open(\"results.pkl\", \"rb\") as f:\n",
        "      results = pickle.load(f)\n",
        "  # Initialize all_texts with leaf_texts\n",
        "  all_texts = leaf_texts.copy()\n",
        "\n",
        "  # Iterate through the results to extract summaries from each level and add them to all_texts\n",
        "  for level in sorted(results.keys()):\n",
        "    # Extract summaries from the current level's DataFrame\n",
        "    summaries = results[level][1][\"summaries\"].tolist()\n",
        "    # Extend all_texts with the summaries from the current level\n",
        "    all_texts.extend(summaries)\n",
        "\n",
        "  # Now, use all_texts to build the vectorstore with Chroma\n",
        "  vectorstore = Chroma.from_texts(texts=all_texts, embedding=embd)\n",
        "  retriever = vectorstore.as_retriever()\n",
        "\n",
        "  # Prompt\n",
        "  prompt = hub.pull(\"rlm/rag-prompt\")\n",
        "\n",
        "\n",
        "  # Post-processing (joining all docs retrieved after vector search)\n",
        "  def format_docs(docs):\n",
        "      return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "  # Chain\n",
        "  st.session_state.rag_chain = (\n",
        "      {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "      | prompt\n",
        "      | model\n",
        "      | StrOutputParser()\n",
        "  )\n",
        "else:\n",
        "  # Retrieve the chain from session state\n",
        "  rag_chain = st.session_state.rag_chain\n",
        "\n",
        "\n",
        "# Display chat messages from history on app return\n",
        "for message in st.session_state.messages:\n",
        "    with st.chat_message(message['role']):\n",
        "        st.markdown(message['content'])\n",
        "\n",
        "\n",
        "# React to user input\n",
        "if prompt := st.chat_input(\"What's up?\"):\n",
        "    # Display user message in chat message container\n",
        "    with st.chat_message(\"user\"):\n",
        "        st.markdown(prompt)\n",
        "    # Add user message to chat history\n",
        "    st.session_state.messages.append({'role': \"user\", \"content\": prompt})\n",
        "\n",
        "    # Invoke the RAG chain to get the response\n",
        "    ans = rag_chain.invoke(prompt)\n",
        "\n",
        "    # Display assistant response in chat message container\n",
        "    with st.chat_message(\"assistant\"):\n",
        "        st.markdown(ans)\n",
        "    # Add assistant response to chat history\n",
        "    st.session_state.messages.append({\"role\": \"assistant\",\n",
        "                                      \"content\": ans})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_HRqNp_e1EB2",
        "outputId": "8916fb82-52f0-4f7e-f5ca-8d62f4114e6b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "34.106.176.117"
          ]
        }
      ],
      "source": [
        "# Start the app locally\n",
        "!streamlit run app.py &>/content/logs.txt & curl https://loca.lt/mytunnelpassword"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yxvrG0UF1ON9",
        "outputId": "9cd472b1-702c-47a4-ccb4-c8db95f61b56"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[K\u001b[?25hnpx: installed 22 in 2.275s\n",
            "your url is: https://good-papers-stand.loca.lt\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "# Expose the app to the world\n",
        "!npx localtunnel --port 8501"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_gAPrUwcvjy"
      },
      "source": [
        "# 8) Conclusion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "P7OdBOzGPo9F"
      },
      "outputs": [],
      "source": [
        "query = \"Tell me about the topics discussed at Inventures\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kqjl4fNetiRf"
      },
      "source": [
        "<pr> Let's review our work through a lense that I learnt recently from, which is <strong> Problem-Solution-Benefits </strong> </pr>\n",
        "<br>\n",
        "\n",
        "<h2>Problem; Beat it or join it! </h2>\n",
        "\n",
        "<br>\n",
        "<h2>Solution; We implemented RAG yet can go further! </h2>\n",
        "\n",
        "<br>\n",
        "<h2>Benefits; Data retrieval and analysis never gonna go out of style!</h2>"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "XArnMsdopGNf",
        "dUzl7VO7rA-o",
        "V7tNhbFfcRT8"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
