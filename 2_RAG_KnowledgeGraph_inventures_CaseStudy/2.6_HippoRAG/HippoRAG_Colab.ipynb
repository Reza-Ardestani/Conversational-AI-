{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "# Running HippoRAG on Colab\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "DNkIncBfaOsn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#https://stackoverflow.com/questions/76837647/how-to-downgrade-python-version-to-3-9-in-colab-without-creating-virtual-environ\n",
        "#https://github.com/OSU-NLP-Group/HippoRAG/issues/3\n",
        "!pip install -q condacolab\n",
        "import condacolab\n",
        "condacolab.install()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jfiJ6idRlwHd",
        "outputId": "3086072c-c6dd-4358-ec76-e2ce0f79e989"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⏬ Downloading https://github.com/conda-forge/miniforge/releases/download/23.11.0-0/Mambaforge-23.11.0-0-Linux-x86_64.sh...\n",
            "📦 Installing...\n",
            "📌 Adjusting configuration...\n",
            "🩹 Patching environment...\n",
            "⏲ Done in 0:00:11\n",
            "🔁 Restarting kernel...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!conda create --name hippoEnv python=3.9"
      ],
      "metadata": {
        "id": "IOxFJwoel1uX",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b877569-ca1e-4f93-b2a8-b61380a91066"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Channels:\n",
            " - conda-forge\n",
            "Platform: linux-64\n",
            "Collecting package metadata (repodata.json): - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\bdone\n",
            "Solving environment: / \b\b- \b\bdone\n",
            "\n",
            "\n",
            "==> WARNING: A newer version of conda exists. <==\n",
            "    current version: 23.11.0\n",
            "    latest version: 24.5.0\n",
            "\n",
            "Please update conda by running\n",
            "\n",
            "    $ conda update -n base -c conda-forge conda\n",
            "\n",
            "\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local/envs/hippoEnv\n",
            "\n",
            "  added / updated specs:\n",
            "    - python=3.9\n",
            "\n",
            "\n",
            "The following packages will be downloaded:\n",
            "\n",
            "    package                    |            build\n",
            "    ---------------------------|-----------------\n",
            "    bzip2-1.0.8                |       h4bc722e_7         247 KB  conda-forge\n",
            "    ca-certificates-2024.7.4   |       hbcca054_0         151 KB  conda-forge\n",
            "    ld_impl_linux-64-2.40      |       hf3520f5_7         691 KB  conda-forge\n",
            "    libgcc-ng-14.1.0           |       h77fa898_0         822 KB  conda-forge\n",
            "    libgomp-14.1.0             |       h77fa898_0         446 KB  conda-forge\n",
            "    libsqlite-3.46.0           |       hde9e2c9_0         845 KB  conda-forge\n",
            "    libxcrypt-4.4.36           |       hd590300_1          98 KB  conda-forge\n",
            "    libzlib-1.3.1              |       h4ab18f5_1          60 KB  conda-forge\n",
            "    ncurses-6.5                |       h59595ed_0         867 KB  conda-forge\n",
            "    openssl-3.3.1              |       h4ab18f5_1         2.8 MB  conda-forge\n",
            "    pip-24.0                   |     pyhd8ed1ab_0         1.3 MB  conda-forge\n",
            "    python-3.9.19              |h0755675_0_cpython        22.7 MB  conda-forge\n",
            "    setuptools-70.3.0          |     pyhd8ed1ab_0         485 KB  conda-forge\n",
            "    tzdata-2024a               |       h0c530f3_0         117 KB  conda-forge\n",
            "    wheel-0.43.0               |     pyhd8ed1ab_1          57 KB  conda-forge\n",
            "    ------------------------------------------------------------\n",
            "                                           Total:        31.6 MB\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  _libgcc_mutex      conda-forge/linux-64::_libgcc_mutex-0.1-conda_forge \n",
            "  _openmp_mutex      conda-forge/linux-64::_openmp_mutex-4.5-2_gnu \n",
            "  bzip2              conda-forge/linux-64::bzip2-1.0.8-h4bc722e_7 \n",
            "  ca-certificates    conda-forge/linux-64::ca-certificates-2024.7.4-hbcca054_0 \n",
            "  ld_impl_linux-64   conda-forge/linux-64::ld_impl_linux-64-2.40-hf3520f5_7 \n",
            "  libffi             conda-forge/linux-64::libffi-3.4.2-h7f98852_5 \n",
            "  libgcc-ng          conda-forge/linux-64::libgcc-ng-14.1.0-h77fa898_0 \n",
            "  libgomp            conda-forge/linux-64::libgomp-14.1.0-h77fa898_0 \n",
            "  libnsl             conda-forge/linux-64::libnsl-2.0.1-hd590300_0 \n",
            "  libsqlite          conda-forge/linux-64::libsqlite-3.46.0-hde9e2c9_0 \n",
            "  libuuid            conda-forge/linux-64::libuuid-2.38.1-h0b41bf4_0 \n",
            "  libxcrypt          conda-forge/linux-64::libxcrypt-4.4.36-hd590300_1 \n",
            "  libzlib            conda-forge/linux-64::libzlib-1.3.1-h4ab18f5_1 \n",
            "  ncurses            conda-forge/linux-64::ncurses-6.5-h59595ed_0 \n",
            "  openssl            conda-forge/linux-64::openssl-3.3.1-h4ab18f5_1 \n",
            "  pip                conda-forge/noarch::pip-24.0-pyhd8ed1ab_0 \n",
            "  python             conda-forge/linux-64::python-3.9.19-h0755675_0_cpython \n",
            "  readline           conda-forge/linux-64::readline-8.2-h8228510_1 \n",
            "  setuptools         conda-forge/noarch::setuptools-70.3.0-pyhd8ed1ab_0 \n",
            "  tk                 conda-forge/linux-64::tk-8.6.13-noxft_h4845f30_101 \n",
            "  tzdata             conda-forge/noarch::tzdata-2024a-h0c530f3_0 \n",
            "  wheel              conda-forge/noarch::wheel-0.43.0-pyhd8ed1ab_1 \n",
            "  xz                 conda-forge/linux-64::xz-5.2.6-h166bdaf_0 \n",
            "\n",
            "\n",
            "\n",
            "Downloading and Extracting Packages:\n",
            "python-3.9.19        | 22.7 MB   | :   0% 0/1 [00:00<?, ?it/s]\n",
            "openssl-3.3.1        | 2.8 MB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "pip-24.0             | 1.3 MB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "ncurses-6.5          | 867 KB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "libsqlite-3.46.0     | 845 KB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libgcc-ng-14.1.0     | 822 KB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "ld_impl_linux-64-2.4 | 691 KB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "setuptools-70.3.0    | 485 KB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libgomp-14.1.0       | 446 KB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "bzip2-1.0.8          | 247 KB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "ca-certificates-2024 | 151 KB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "tzdata-2024a         | 117 KB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libxcrypt-4.4.36     | 98 KB     | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libzlib-1.3.1        | 60 KB     | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "wheel-0.43.0         | 57 KB     | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "openssl-3.3.1        | 2.8 MB    | :   1% 0.011312534307345483/1 [00:00<00:08,  9.04s/it]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "libsqlite-3.46.0     | 845 KB    | :   4% 0.03786693415119501/1 [00:00<00:02,  2.66s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "python-3.9.19        | 22.7 MB   | :   0% 0.0006883873086152822/1 [00:00<02:54, 174.35s/it]\n",
            "\n",
            "\n",
            "ncurses-6.5          | 867 KB    | :   2% 0.01846157313246156/1 [00:00<00:06,  6.42s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "python-3.9.19        | 22.7 MB   | :  11% 0.11358390592152158/1 [00:00<00:01,  1.63s/it]   \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "ld_impl_linux-64-2.4 | 691 KB    | :   2% 0.02315425903262003/1 [00:00<00:09,  9.39s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "libsqlite-3.46.0     | 845 KB    | : 100% 1.0/1 [00:00<00:00,  5.21it/s]                \u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "libsqlite-3.46.0     | 845 KB    | : 100% 1.0/1 [00:00<00:00,  5.21it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "setuptools-70.3.0    | 485 KB    | :   3% 0.03300211701812659/1 [00:00<00:06,  6.77s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libgomp-14.1.0       | 446 KB    | :   4% 0.035857088143568416/1 [00:00<00:06,  6.86s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "bzip2-1.0.8          | 247 KB    | :   6% 0.06481448515129577/1 [00:00<00:03,  4.13s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "ca-certificates-2024 | 151 KB    | :  11% 0.10580356854565297/1 [00:00<00:02,  2.64s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "tzdata-2024a         | 117 KB    | :  14% 0.13674414722697492/1 [00:00<00:01,  2.05s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "python-3.9.19        | 22.7 MB   | :  27% 0.26571750112549897/1 [00:00<00:00,  1.01it/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libzlib-1.3.1        | 60 KB     | :  27% 0.2660863351414558/1 [00:00<00:00,  1.18s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "python-3.9.19        | 22.7 MB   | :  96% 0.9589235209010882/1 [00:00<00:00,  1.97it/s]\n",
            "\n",
            "pip-24.0             | 1.3 MB    | : 100% 1.0/1 [00:00<00:00,  1.04it/s]                 \u001b[A\u001b[A\n",
            "\n",
            "pip-24.0             | 1.3 MB    | : 100% 1.0/1 [00:00<00:00,  1.04it/s]\u001b[A\u001b[A\n",
            "openssl-3.3.1        | 2.8 MB    | : 100% 1.0/1 [00:01<00:00,  1.26s/it]                 \u001b[A\n",
            "openssl-3.3.1        | 2.8 MB    | : 100% 1.0/1 [00:01<00:00,  1.26s/it]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libgcc-ng-14.1.0     | 822 KB    | : 100% 1.0/1 [00:01<00:00,  1.36s/it]                 \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libgcc-ng-14.1.0     | 822 KB    | : 100% 1.0/1 [00:01<00:00,  1.36s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "setuptools-70.3.0    | 485 KB    | : 100% 1.0/1 [00:01<00:00,  1.64s/it]                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "setuptools-70.3.0    | 485 KB    | : 100% 1.0/1 [00:01<00:00,  1.64s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "ld_impl_linux-64-2.4 | 691 KB    | : 100% 1.0/1 [00:01<00:00,  1.71s/it]                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "ld_impl_linux-64-2.4 | 691 KB    | : 100% 1.0/1 [00:01<00:00,  1.71s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libgomp-14.1.0       | 446 KB    | : 100% 1.0/1 [00:01<00:00,  1.80s/it]                 \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libgomp-14.1.0       | 446 KB    | : 100% 1.0/1 [00:01<00:00,  1.80s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "bzip2-1.0.8          | 247 KB    | : 100% 1.0/1 [00:01<00:00,  1.85s/it]                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "bzip2-1.0.8          | 247 KB    | : 100% 1.0/1 [00:01<00:00,  1.85s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "ca-certificates-2024 | 151 KB    | : 100% 1.0/1 [00:01<00:00,  1.89s/it]                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "ca-certificates-2024 | 151 KB    | : 100% 1.0/1 [00:01<00:00,  1.89s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "ncurses-6.5          | 867 KB    | : 100% 1.0/1 [00:02<00:00,  2.36s/it]                \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "ncurses-6.5          | 867 KB    | : 100% 1.0/1 [00:02<00:00,  2.36s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libxcrypt-4.4.36     | 98 KB     | : 100% 1.0/1 [00:02<00:00,  2.43s/it]              \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libxcrypt-4.4.36     | 98 KB     | : 100% 1.0/1 [00:02<00:00,  2.43s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "tzdata-2024a         | 117 KB    | : 100% 1.0/1 [00:02<00:00,  2.43s/it]                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "tzdata-2024a         | 117 KB    | : 100% 1.0/1 [00:02<00:00,  2.43s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libzlib-1.3.1        | 60 KB     | : 100% 1.0/1 [00:02<00:00,  2.53s/it]               \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libzlib-1.3.1        | 60 KB     | : 100% 1.0/1 [00:02<00:00,  2.53s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "wheel-0.43.0         | 57 KB     | : 100% 1.0/1 [00:02<00:00,  2.57s/it]               \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \n",
            "                                                                        \u001b[A\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "Preparing transaction: | \b\b/ \b\b- \b\bdone\n",
            "Verifying transaction: | \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\bdone\n",
            "Executing transaction: - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\bdone\n",
            "#\n",
            "# To activate this environment, use\n",
            "#\n",
            "#     $ conda activate hippoEnv\n",
            "#\n",
            "# To deactivate an active environment, use\n",
            "#\n",
            "#     $ conda deactivate\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "eval \"$(conda shell.bash hook)\"\n",
        "conda activate hippoEnv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6WNMRzg4l-Ku",
        "outputId": "e50a3ddf-2cc6-46a5-83e7-fec8f3198e16"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 --version\n",
        "# python is still 3.10 but now I can pip install requeriments.txt without crash"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j81yqQGixDb0",
        "outputId": "36ebb802-0f77-410a-e6a1-2f48171de564"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.10.13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mDoLjGZ8ZX7S",
        "outputId": "d1e3c3a2-cb8c-4e1d-897a-337bd61d7531"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'HippoRAG'...\n",
            "remote: Enumerating objects: 440, done.\u001b[K\n",
            "remote: Counting objects: 100% (196/196), done.\u001b[K\n",
            "remote: Compressing objects: 100% (62/62), done.\u001b[K\n",
            "remote: Total 440 (delta 170), reused 134 (delta 134), pack-reused 244\u001b[K\n",
            "Receiving objects: 100% (440/440), 68.26 MiB | 20.62 MiB/s, done.\n",
            "Resolving deltas: 100% (240/240), done.\n",
            "Updating files: 100% (114/114), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/OSU-NLP-Group/HippoRAG.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# older versions of huggingface cuase the following error: 'split_torch_state_dict_into_shards'\n",
        "!sed -i 's/huggingface-hub==0.20.3/huggingface-hub==0.23.4/' /content/HippoRAG/requirements.txt\n",
        "\n"
      ],
      "metadata": {
        "id": "BYjaFOSA3zOh"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# remove this packages from requirements.txt to avoid colab crash\n",
        "!grep -Ev '^(ipython|ipykernel|tornado|prompt-toolkit|pyzmq)' /content/HippoRAG/requirements.txt > /content/HippoRAG/new_requirements.txt\n",
        "!pip install -r /content/HippoRAG/new_requirements.txt"
      ],
      "metadata": {
        "id": "RJpfn7qcagK2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        },
        "outputId": "a4c1a09a-1e76-4e99-cc3d-9600dafd282d",
        "collapsed": true
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully installed Flask-3.0.2 GitPython-3.1.42 Jinja2-3.1.3 MarkupSafe-2.1.5 PyYAML-6.0.1 Pygments-2.17.2 SQLAlchemy-2.0.31 Unidecode-1.3.8 Werkzeug-3.0.1 accelerate-0.32.1 aiohttp-3.9.3 aiosignal-1.3.1 annotated-types-0.6.0 anyio-4.3.0 asttokens-2.4.1 async-timeout-4.0.3 attrs-23.2.0 backcall-0.2.0 beautifulsoup4-4.12.3 bitarray-2.9.2 blinker-1.7.0 bs4-0.0.2 certifi-2024.7.4 click-8.1.7 colbert-ai-0.2.19 comm-0.2.1 contourpy-1.2.0 cycler-0.12.1 dataclasses-json-0.6.7 datasets-2.17.1 debugpy-1.8.0 decorator-5.1.1 dill-0.3.8 distro-1.9.0 docker-pycreds-0.4.0 editdistance-0.8.1 elastic-transport-8.12.0 elasticsearch-8.12.1 entrypoints-0.4 eval_type_backport-0.2.0 exceptiongroup-1.2.0 executing-2.0.1 faiss-gpu-1.7.2 filelock-3.13.1 fonttools-4.49.0 frozenlist-1.4.1 fsspec-2023.10.0 git-python-1.0.3 gitdb-4.0.11 greenlet-3.0.3 gritlm-1.0.1 h11-0.14.0 httpcore-1.0.3 httpx-0.26.0 huggingface-hub-0.23.4 igraph-0.11.4 importlib-metadata-7.0.1 importlib_resources-6.1.2 ipdb-0.13.13 ipython-8.26.0 itsdangerous-2.1.2 jedi-0.18.1 joblib-1.3.2 jsonlines-4.0.0 jupyter_client-8.6.0 jupyter_core-5.7.1 kiwisolver-1.4.5 langchain-0.1.20 langchain-core-0.1.52 langchain-openai-0.1.5 langchain-text-splitters-0.0.2 langchain-together-0.1.2 langchain_community-0.0.38 langsmith-0.1.85 markdown-it-py-3.0.0 marshmallow-3.21.3 matplotlib-3.8.3 matplotlib-inline-0.1.6 mdurl-0.1.2 mteb-1.7.49 multidict-6.0.5 multiprocess-0.70.16 mypy-extensions-1.0.0 nest-asyncio-1.5.8 ninja-1.11.1.1 nltk-3.8.1 numpy-1.26.4 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 openai-1.12.0 orjson-3.10.6 pandas-2.2.0 parso-0.8.3 pexpect-4.8.0 pickleshare-0.7.5 pillow-10.3.0 prompt-toolkit-3.0.47 protobuf-5.27.2 psutil-5.9.7 ptyprocess-0.7.0 pure-eval-0.2.2 pyarrow-15.0.0 pyarrow-hotfix-0.6 pydantic-2.7.1 pydantic_core-2.18.2 pyparsing-3.1.1 python-dateutil-2.8.2 python-dotenv-1.0.1 pytrec-eval-terrier-0.5.6 pytrec_eval-0.5 pytz-2024.1 pyzmq-26.0.3 rank-bm25-0.2.2 rapidfuzz-3.6.1 regex-2023.12.25 rich-13.7.1 safetensors-0.4.2 scikit-learn-1.4.1.post1 scipy-1.12.0 sentence-transformers-2.6.0 sentencepiece-0.2.0 sentry-sdk-2.9.0 setproctitle-1.3.3 setuptools-69.5.1 shellingham-1.5.4 six-1.16.0 smmap-5.0.1 sniffio-1.3.0 soupsieve-2.5 stack-data-0.6.3 tabulate-0.9.0 tenacity-8.5.0 texttable-1.7.0 thefuzz-0.22.1 threadpoolctl-3.3.0 tiktoken-0.6.0 together-1.1.4 tokenizers-0.15.2 tomli-2.0.1 torch-1.13.1 tornado-6.4.1 tqdm-4.66.2 traitlets-5.14.1 transformers-4.37.2 typer-0.12.3 typing-inspect-0.9.0 typing_extensions-4.9.0 tzdata-2024.1 ujson-5.9.0 urllib3-2.2.1 wandb-0.17.4 wcwidth-0.2.5 xxhash-3.4.1 yarl-1.9.4 zipp-3.17.0\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "certifi",
                  "cycler",
                  "dateutil",
                  "kiwisolver",
                  "pickleshare",
                  "prompt_toolkit",
                  "six"
                ]
              },
              "id": "f4522df4c2924f3798c2e07465d9690b"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# to use Colbert uncomment\n",
        "!cd /content/HippoRAG/exp; wget https://downloads.cs.stanford.edu/nlp/data/colbert/colbertv2/colbertv2.0.tar.gz\n",
        "!cd /content/HippoRAG/exp; tar -xvzf colbertv2.0.tar.gz\n"
      ],
      "metadata": {
        "id": "GaYIfMCHry9F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b60fb8b-c35b-4cfd-8ee7-2e04d596bdf9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-07-14 21:55:53--  https://downloads.cs.stanford.edu/nlp/data/colbert/colbertv2/colbertv2.0.tar.gz\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 405924985 (387M) [application/octet-stream]\n",
            "Saving to: ‘colbertv2.0.tar.gz’\n",
            "\n",
            "colbertv2.0.tar.gz  100%[===================>] 387.12M  5.16MB/s    in 73s     \n",
            "\n",
            "2024-07-14 21:57:07 (5.31 MB/s) - ‘colbertv2.0.tar.gz’ saved [405924985/405924985]\n",
            "\n",
            "colbertv2.0/\n",
            "colbertv2.0/artifact.metadata\n",
            "colbertv2.0/vocab.txt\n",
            "colbertv2.0/tokenizer.json\n",
            "colbertv2.0/special_tokens_map.json\n",
            "colbertv2.0/tokenizer_config.json\n",
            "colbertv2.0/config.json\n",
            "colbertv2.0/pytorch_model.bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create a new secret on colab to set OPENAI_API_KEY\n",
        "from google.colab import userdata\n",
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
        "os.environ[\"TOGETHER_API_KEY\"] = userdata.get('TOGETHER_API_KEY')\n"
      ],
      "metadata": {
        "id": "93hm2TPEa0lV"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# If you want langsmith logging uncomment\n",
        "\n",
        "# from uuid import uuid4\n",
        "# unique_id = uuid4().hex[0:8]\n",
        "# os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "# os.environ[\"LANGCHAIN_PROJECT\"] = f\"HippoRagLoad - {unique_id}\"\n",
        "# os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
        "# os.environ[\"LANGCHAIN_API_KEY\"] = \"LANGCHAIN_API_KEY\"  # Update to your API key"
      ],
      "metadata": {
        "id": "sBswYl2Vzupk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fix create_graph.py nltk download path\n",
        "!if [ \"$(head -n 1 /content/HippoRAG/src/create_graph.py)\" != \"import nltk\" ]; then sed -i '1i\\nltk.download(\"stopwords\", download_dir=\"/tmp\")' /content/HippoRAG/src/create_graph.py; sed -i '1i\\nltk.data.path.append(\"/tmp/\")' /content/HippoRAG/src/create_graph.py; sed -i '1i\\import nltk' /content/HippoRAG/src/create_graph.py; fi\n",
        "\n"
      ],
      "metadata": {
        "id": "vWTTMMkz6Hcq"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Will use the corpus \"case_study_movies\" in the data folder with only 1 passages and Contriever\n",
        "\n",
        "%env data=inventures2024\n",
        "%env HF_RETRIEVER=colbertv2\n",
        "# %env LLM_MODEL=openchat/openchat-3.5-1210\n",
        "%env LLM_MODEL=gpt-3.5-turbo-1106\n",
        "%env SYNONYM_THRESH=0.8\n",
        "# GPUS=0,1,2,3\n",
        "%env GPUS=0\n",
        "# LLM API provider e.g., 'openai', 'together', see 'src/langchain_util.py'\n",
        "%env LLM_API=openai\n",
        "%env extraction_type=ner\n",
        "# 'all' or 'the number of passages to use'\n",
        "%env num_passages=4\n",
        "\n",
        "# bash src/setup_hipporag.sh $DATA $HF_RETRIEVER $LLM $GPUS $SYNONYM_THRESH $LLM_API\n",
        "# here are the commands inside setup_hipporag.sh, will run here to pass num_passages to 3\n",
        "\n",
        "!echo ***Extracting Open Information***\n",
        "# Running Open Information Extraction\n",
        "!cd /content/HippoRAG; python src/openie_with_retrieval_option_parallel.py --dataset $data --llm $LLM_API --model_name $LLM_MODEL --run_ner --num_passages $num_passages # NER and OpenIE for passages\n",
        "# !echo ***Extracting entities from ***\n",
        "# !cd /content/HippoRAG; python src/named_entity_extraction_parallel.py --num_processes 1 --dataset $data --llm $LLM_API --model_name $LLM_MODEL  # NER for queries\n",
        "\n",
        "!echo ***Creating ColBERT Graph***\n",
        "# Creating ColBERT Graph\n",
        "!cd /content/HippoRAG; python src/create_graph.py --dataset $data --model_name colbertv2 --extraction_model $LLM_MODEL --threshold $SYNONYM_THRESH --extraction_type $extraction_type --cosine_sim_edges\n",
        "\n",
        "\n",
        "# Getting Nearest Neighbor Files\n",
        "%env CUDA_VISIBLE_DEVICES=0\n",
        "!echo ***Creating nearest neighbor file output/kb_to_kb.tsv ***\n",
        "!cd /content/HippoRAG; python src/colbertv2_knn.py --filename output/kb_to_kb.tsv\n",
        "!echo ***Creating nearest neighbor file output/query_to_kb.tsv***\n",
        "!cd /content/HippoRAG; python src/colbertv2_knn.py --filename output/query_to_kb.tsv\n",
        "\n",
        "\n",
        "!echo ***Creating graph for the second time...\n",
        "!cd /content/HippoRAG; python src/create_graph.py --dataset $data --model_name colbertv2 --extraction_model $LLM_MODEL --threshold $SYNONYM_THRESH --create_graph --extraction_type $extraction_type --cosine_sim_edges\n",
        "\n",
        "\n",
        "!echo ***Creating colbert indexing files...\n",
        "!cd /content/HippoRAG; python3 src/colbertv2_indexing.py --dataset $data --phrase output/$data'_facts_and_sim_graph_phrase_dict_ents_only_lower_preprocess_ner.v3.subset.p' --corpus 'data/'$data'_corpus.json'\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wg-cmt3dzMPB",
        "outputId": "5c442995-de97-430c-cd32-b0778ac7ae59"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: data=inventures2024\n",
            "env: HF_RETRIEVER=colbertv2\n",
            "env: LLM_MODEL=gpt-3.5-turbo-1106\n",
            "env: SYNONYM_THRESH=0.8\n",
            "env: GPUS=0\n",
            "env: LLM_API=openai\n",
            "env: extraction_type=ner\n",
            "env: num_passages=4\n",
            "***Extracting Open Information***\n",
            "ner_gpt-3.5-turbo-1106_4\n",
            "0it [00:00, ?it/s]\n",
            "0it [00:00, ?it/s]\n",
            "0it [00:00, ?it/s]\n",
            "0it [00:00, ?it/s]\n",
            "0it [00:00, ?it/s]\n",
            "0it [00:00, ?it/s]\n",
            "100% 1/1 [00:02<00:00,  2.57s/it]\n",
            "100% 1/1 [00:02<00:00,  2.71s/it]\n",
            "100% 1/1 [00:03<00:00,  3.02s/it]\n",
            "100% 1/1 [00:05<00:00,  5.57s/it]\n",
            "/usr/local/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
            "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
            "/usr/local/lib/python3.10/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  ret = ret.dtype.type(ret / rcount)\n",
            "OpenIE saved to output/openie_inventures2024_results_ner_gpt-3.5-turbo-1106_4.json\n",
            "***Creating ColBERT Graph***\n",
            "[nltk_data] Downloading package stopwords to /tmp...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "100% 4/4 [00:00<00:00, 7034.47it/s]\n",
            "Correct Wiki Format: 0 out of 4\n",
            "0it [00:00, ?it/s]\n",
            "env: CUDA_VISIBLE_DEVICES=0\n",
            "***Creating nearest neighbor file output/kb_to_kb.tsv condacolab_install.log HippoRAG sample_data\n",
            "\n",
            "\n",
            "[Jul 14, 21:59:06] #> Creating directory colbert/indexes/nbits_2 \n",
            "\n",
            "\n",
            "#> Starting...\n",
            "nranks = 1 \t num_gpus = 1 \t device=0\n",
            "{\n",
            "    \"query_token_id\": \"[unused0]\",\n",
            "    \"doc_token_id\": \"[unused1]\",\n",
            "    \"query_token\": \"[Q]\",\n",
            "    \"doc_token\": \"[D]\",\n",
            "    \"ncells\": null,\n",
            "    \"centroid_score_threshold\": null,\n",
            "    \"ndocs\": null,\n",
            "    \"load_index_with_mmap\": false,\n",
            "    \"index_path\": null,\n",
            "    \"index_bsize\": 64,\n",
            "    \"nbits\": 2,\n",
            "    \"kmeans_niters\": 20,\n",
            "    \"resume\": false,\n",
            "    \"similarity\": \"cosine\",\n",
            "    \"bsize\": 64,\n",
            "    \"accumsteps\": 1,\n",
            "    \"lr\": 1e-5,\n",
            "    \"maxsteps\": 400000,\n",
            "    \"save_every\": null,\n",
            "    \"warmup\": 20000,\n",
            "    \"warmup_bert\": null,\n",
            "    \"relu\": false,\n",
            "    \"nway\": 64,\n",
            "    \"use_ib_negatives\": true,\n",
            "    \"reranker\": false,\n",
            "    \"distillation_alpha\": 1.0,\n",
            "    \"ignore_scores\": false,\n",
            "    \"model_name\": null,\n",
            "    \"query_maxlen\": 32,\n",
            "    \"attend_to_mask_tokens\": false,\n",
            "    \"interaction\": \"colbert\",\n",
            "    \"dim\": 128,\n",
            "    \"doc_maxlen\": 180,\n",
            "    \"mask_punctuation\": true,\n",
            "    \"checkpoint\": \"exp\\/colbertv2.0\",\n",
            "    \"triples\": \"\\/future\\/u\\/okhattab\\/root\\/unit\\/experiments\\/2021.10\\/downstream.distillation.round2.2_score\\/round2.nway6.cosine.ib\\/examples.64.json\",\n",
            "    \"collection\": \"data\\/lm_vectors\\/colbert\\/corpus.tsv\",\n",
            "    \"queries\": \"\\/future\\/u\\/okhattab\\/data\\/MSMARCO\\/queries.train.tsv\",\n",
            "    \"index_name\": \"nbits_2\",\n",
            "    \"overwrite\": false,\n",
            "    \"root\": \"\",\n",
            "    \"experiment\": \"colbert\",\n",
            "    \"index_root\": null,\n",
            "    \"name\": \"2024-07\\/14\\/21.59.05\",\n",
            "    \"rank\": 0,\n",
            "    \"nranks\": 1,\n",
            "    \"amp\": true,\n",
            "    \"gpus\": 1,\n",
            "    \"avoid_fork_if_possible\": false\n",
            "}\n",
            "[Jul 14, 21:59:11] #> Loading collection...\n",
            "0M \n",
            "[Jul 14, 21:59:14] [0] \t\t # of sampled PIDs = 50 \t sampled_pids[:3] = [26, 46, 0]\n",
            "[Jul 14, 21:59:14] [0] \t\t #> Encoding 50 passages..\n",
            "[Jul 14, 21:59:16] [0] \t\t avg_doclen_est = 6.960000038146973 \t len(local_sample) = 50\n",
            "[Jul 14, 21:59:16] [0] \t\t Creating 256 partitions.\n",
            "[Jul 14, 21:59:16] [0] \t\t *Estimated* 348 embeddings.\n",
            "[Jul 14, 21:59:16] [0] \t\t #> Saving the indexing plan to colbert/indexes/nbits_2/plan.json ..\n",
            "WARNING clustering 331 points to 256 centroids: please provide at least 9984 training points\n",
            "Clustering 331 points in 128D to 256 clusters, redo 1 times, 20 iterations\n",
            "  Preprocessing in 0.00 s\n",
            "  Iteration 19 (0.01 s, search 0.01 s): objective=10.5927 imbalance=1.185 nsplit=0       \n",
            "[Jul 14, 21:59:17] Loading decompress_residuals_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n",
            "[Jul 14, 22:00:44] Loading packbits_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n",
            "[0.03, 0.03, 0.035, 0.04, 0.026, 0.035, 0.03, 0.039, 0.041, 0.038, 0.044, 0.035, 0.028, 0.045, 0.044, 0.031, 0.056, 0.031, 0.041, 0.037, 0.048, 0.032, 0.037, 0.046, 0.033, 0.025, 0.026, 0.043, 0.02, 0.026, 0.03, 0.049, 0.037, 0.038, 0.03, 0.04, 0.037, 0.03, 0.032, 0.035, 0.036, 0.032, 0.028, 0.04, 0.032, 0.037, 0.049, 0.019, 0.032, 0.045, 0.021, 0.029, 0.041, 0.047, 0.05, 0.068, 0.039, 0.03, 0.034, 0.025, 0.053, 0.035, 0.048, 0.032, 0.03, 0.036, 0.028, 0.036, 0.04, 0.048, 0.033, 0.058, 0.048, 0.039, 0.028, 0.036, 0.05, 0.041, 0.038, 0.03, 0.034, 0.032, 0.039, 0.037, 0.041, 0.037, 0.062, 0.043, 0.056, 0.044, 0.034, 0.034, 0.045, 0.031, 0.043, 0.041, 0.031, 0.043, 0.043, 0.04, 0.022, 0.043, 0.037, 0.038, 0.029, 0.033, 0.022, 0.042, 0.034, 0.034, 0.044, 0.025, 0.053, 0.032, 0.043, 0.029, 0.038, 0.033, 0.032, 0.023, 0.028, 0.04, 0.033, 0.039, 0.035, 0.051, 0.024, 0.039]\n",
            "[Jul 14, 22:02:09] #> Got bucket_cutoffs_quantiles = tensor([0.2500, 0.5000, 0.7500], device='cuda:0') and bucket_weights_quantiles = tensor([0.1250, 0.3750, 0.6250, 0.8750], device='cuda:0')\n",
            "[Jul 14, 22:02:09] #> Got bucket_cutoffs = tensor([-0.0262, -0.0015,  0.0261], device='cuda:0') and bucket_weights = tensor([-0.0473, -0.0126,  0.0108,  0.0486], device='cuda:0')\n",
            "[Jul 14, 22:02:09] avg_residual = 0.037017822265625\n",
            "0it [00:00, ?it/s][Jul 14, 22:02:09] [0] \t\t #> Encoding 50 passages..\n",
            "[Jul 14, 22:02:09] [0] \t\t #> Saving chunk 0: \t 50 passages and 348 embeddings. From #0 onward.\n",
            "1it [00:00, 20.05it/s]\n",
            "[Jul 14, 22:02:09] [0] \t\t #> Checking all files were saved...\n",
            "[Jul 14, 22:02:09] [0] \t\t Found all files!\n",
            "[Jul 14, 22:02:09] [0] \t\t #> Building IVF...\n",
            "[Jul 14, 22:02:09] [0] \t\t #> Loading codes...\n",
            "100% 1/1 [00:00<00:00, 3228.87it/s]\n",
            "[Jul 14, 22:02:09] [0] \t\t Sorting codes...\n",
            "[Jul 14, 22:02:09] [0] \t\t Getting unique codes...\n",
            "[Jul 14, 22:02:09] #> Optimizing IVF to store map from centroids to list of pids..\n",
            "[Jul 14, 22:02:09] #> Building the emb2pid mapping..\n",
            "[Jul 14, 22:02:09] len(emb2pid) = 348\n",
            "100% 256/256 [00:00<00:00, 147330.11it/s]\n",
            "[Jul 14, 22:02:09] #> Saved optimized IVF to colbert/indexes/nbits_2/ivf.pid.pt\n",
            "[Jul 14, 22:02:09] [0] \t\t #> Saving the indexing metadata to colbert/indexes/nbits_2/metadata.json ..\n",
            "#> Joined...\n",
            "[Jul 14, 22:02:10] #> Loading collection...\n",
            "0M \n",
            "[Jul 14, 22:02:13] #> Loading codec...\n",
            "[Jul 14, 22:02:13] Loading decompress_residuals_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n",
            "[Jul 14, 22:02:13] Loading packbits_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n",
            "[Jul 14, 22:02:13] #> Loading IVF...\n",
            "[Jul 14, 22:02:13] #> Loading doclens...\n",
            "100% 1/1 [00:00<00:00, 8594.89it/s]\n",
            "[Jul 14, 22:02:13] #> Loading codes and residuals...\n",
            "100% 1/1 [00:00<00:00, 1399.03it/s]\n",
            "[Jul 14, 22:02:13] #> Loading the queries from data/lm_vectors/colbert/queries.tsv ...\n",
            "[Jul 14, 22:02:13] #> Got 50 queries. All QIDs are unique.\n",
            "\n",
            "\n",
            "#> QueryTokenizer.tensorize(batch_text[0], batch_background[0], bsize) ==\n",
            "#> Input: . academia, \t\t True, \t\t None\n",
            "#> Output IDs: torch.Size([32]), tensor([  101,     1, 16926,   102,   103,   103,   103,   103,   103,   103,\n",
            "          103,   103,   103,   103,   103,   103,   103,   103,   103,   103,\n",
            "          103,   103,   103,   103,   103,   103,   103,   103,   103,   103,\n",
            "          103,   103], device='cuda:0')\n",
            "#> Output Mask: torch.Size([32]), tensor([1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
            "\n",
            "50it [00:00, 127.96it/s]\n",
            "Saved nearest neighbors to data/lm_vectors/colbert/nearest_neighbor_kb_to_kb.p\n",
            "***Creating nearest neighbor file output/query_to_kb.tsv***\n",
            "\n",
            "\n",
            "[Jul 14, 22:02:18] #> Note: Output directory colbert/indexes/nbits_2 already exists\n",
            "\n",
            "\n",
            "[Jul 14, 22:02:18] #> Will delete 10 files already at colbert/indexes/nbits_2 in 20 seconds...\n",
            "#> Starting...\n",
            "nranks = 1 \t num_gpus = 1 \t device=0\n",
            "{\n",
            "    \"query_token_id\": \"[unused0]\",\n",
            "    \"doc_token_id\": \"[unused1]\",\n",
            "    \"query_token\": \"[Q]\",\n",
            "    \"doc_token\": \"[D]\",\n",
            "    \"ncells\": null,\n",
            "    \"centroid_score_threshold\": null,\n",
            "    \"ndocs\": null,\n",
            "    \"load_index_with_mmap\": false,\n",
            "    \"index_path\": null,\n",
            "    \"index_bsize\": 64,\n",
            "    \"nbits\": 2,\n",
            "    \"kmeans_niters\": 20,\n",
            "    \"resume\": false,\n",
            "    \"similarity\": \"cosine\",\n",
            "    \"bsize\": 64,\n",
            "    \"accumsteps\": 1,\n",
            "    \"lr\": 1e-5,\n",
            "    \"maxsteps\": 400000,\n",
            "    \"save_every\": null,\n",
            "    \"warmup\": 20000,\n",
            "    \"warmup_bert\": null,\n",
            "    \"relu\": false,\n",
            "    \"nway\": 64,\n",
            "    \"use_ib_negatives\": true,\n",
            "    \"reranker\": false,\n",
            "    \"distillation_alpha\": 1.0,\n",
            "    \"ignore_scores\": false,\n",
            "    \"model_name\": null,\n",
            "    \"query_maxlen\": 32,\n",
            "    \"attend_to_mask_tokens\": false,\n",
            "    \"interaction\": \"colbert\",\n",
            "    \"dim\": 128,\n",
            "    \"doc_maxlen\": 180,\n",
            "    \"mask_punctuation\": true,\n",
            "    \"checkpoint\": \"exp\\/colbertv2.0\",\n",
            "    \"triples\": \"\\/future\\/u\\/okhattab\\/root\\/unit\\/experiments\\/2021.10\\/downstream.distillation.round2.2_score\\/round2.nway6.cosine.ib\\/examples.64.json\",\n",
            "    \"collection\": \"data\\/lm_vectors\\/colbert\\/corpus.tsv\",\n",
            "    \"queries\": \"\\/future\\/u\\/okhattab\\/data\\/MSMARCO\\/queries.train.tsv\",\n",
            "    \"index_name\": \"nbits_2\",\n",
            "    \"overwrite\": false,\n",
            "    \"root\": \"\",\n",
            "    \"experiment\": \"colbert\",\n",
            "    \"index_root\": null,\n",
            "    \"name\": \"2024-07\\/14\\/22.02.17\",\n",
            "    \"rank\": 0,\n",
            "    \"nranks\": 1,\n",
            "    \"amp\": true,\n",
            "    \"gpus\": 1,\n",
            "    \"avoid_fork_if_possible\": false\n",
            "}\n",
            "[Jul 14, 22:02:43] #> Loading collection...\n",
            "0M \n",
            "[Jul 14, 22:02:45] [0] \t\t # of sampled PIDs = 50 \t sampled_pids[:3] = [26, 46, 0]\n",
            "[Jul 14, 22:02:45] [0] \t\t #> Encoding 50 passages..\n",
            "[Jul 14, 22:02:45] [0] \t\t avg_doclen_est = 6.960000038146973 \t len(local_sample) = 50\n",
            "[Jul 14, 22:02:45] [0] \t\t Creating 256 partitions.\n",
            "[Jul 14, 22:02:45] [0] \t\t *Estimated* 348 embeddings.\n",
            "[Jul 14, 22:02:45] [0] \t\t #> Saving the indexing plan to colbert/indexes/nbits_2/plan.json ..\n",
            "WARNING clustering 331 points to 256 centroids: please provide at least 9984 training points\n",
            "Clustering 331 points in 128D to 256 clusters, redo 1 times, 20 iterations\n",
            "  Preprocessing in 0.00 s\n",
            "  Iteration 19 (0.01 s, search 0.00 s): objective=11.2013 imbalance=1.222 nsplit=0       \n",
            "[Jul 14, 22:02:46] Loading decompress_residuals_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n",
            "[Jul 14, 22:02:46] Loading packbits_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n",
            "[0.066, 0.041, 0.026, 0.026, 0.032, 0.04, 0.04, 0.04, 0.045, 0.028, 0.028, 0.038, 0.037, 0.059, 0.027, 0.042, 0.039, 0.041, 0.035, 0.042, 0.058, 0.023, 0.033, 0.039, 0.044, 0.028, 0.036, 0.032, 0.027, 0.034, 0.03, 0.023, 0.029, 0.028, 0.049, 0.032, 0.033, 0.05, 0.034, 0.046, 0.031, 0.05, 0.048, 0.044, 0.043, 0.042, 0.043, 0.051, 0.043, 0.049, 0.048, 0.046, 0.039, 0.038, 0.041, 0.036, 0.031, 0.038, 0.061, 0.039, 0.03, 0.049, 0.053, 0.042, 0.03, 0.037, 0.032, 0.035, 0.028, 0.054, 0.043, 0.038, 0.046, 0.035, 0.045, 0.054, 0.052, 0.046, 0.039, 0.039, 0.032, 0.037, 0.045, 0.04, 0.037, 0.043, 0.047, 0.045, 0.027, 0.069, 0.042, 0.055, 0.04, 0.051, 0.024, 0.038, 0.047, 0.048, 0.036, 0.028, 0.034, 0.03, 0.032, 0.038, 0.042, 0.047, 0.019, 0.033, 0.043, 0.028, 0.026, 0.043, 0.036, 0.03, 0.038, 0.03, 0.044, 0.047, 0.044, 0.039, 0.049, 0.028, 0.029, 0.035, 0.035, 0.032, 0.041, 0.028]\n",
            "[Jul 14, 22:02:46] #> Got bucket_cutoffs_quantiles = tensor([0.2500, 0.5000, 0.7500], device='cuda:0') and bucket_weights_quantiles = tensor([0.1250, 0.3750, 0.6250, 0.8750], device='cuda:0')\n",
            "[Jul 14, 22:02:46] #> Got bucket_cutoffs = tensor([-0.0281, -0.0008,  0.0254], device='cuda:0') and bucket_weights = tensor([-0.0519, -0.0127,  0.0108,  0.0499], device='cuda:0')\n",
            "[Jul 14, 22:02:46] avg_residual = 0.0390625\n",
            "0it [00:00, ?it/s][Jul 14, 22:02:46] [0] \t\t #> Encoding 50 passages..\n",
            "[Jul 14, 22:02:46] [0] \t\t #> Saving chunk 0: \t 50 passages and 348 embeddings. From #0 onward.\n",
            "1it [00:00, 20.32it/s]\n",
            "[Jul 14, 22:02:46] [0] \t\t #> Checking all files were saved...\n",
            "[Jul 14, 22:02:46] [0] \t\t Found all files!\n",
            "[Jul 14, 22:02:46] [0] \t\t #> Building IVF...\n",
            "[Jul 14, 22:02:46] [0] \t\t #> Loading codes...\n",
            "100% 1/1 [00:00<00:00, 2381.77it/s]\n",
            "[Jul 14, 22:02:46] [0] \t\t Sorting codes...\n",
            "[Jul 14, 22:02:46] [0] \t\t Getting unique codes...\n",
            "[Jul 14, 22:02:46] #> Optimizing IVF to store map from centroids to list of pids..\n",
            "[Jul 14, 22:02:46] #> Building the emb2pid mapping..\n",
            "[Jul 14, 22:02:46] len(emb2pid) = 348\n",
            "100% 256/256 [00:00<00:00, 56480.03it/s]\n",
            "[Jul 14, 22:02:46] #> Saved optimized IVF to colbert/indexes/nbits_2/ivf.pid.pt\n",
            "[Jul 14, 22:02:46] [0] \t\t #> Saving the indexing metadata to colbert/indexes/nbits_2/metadata.json ..\n",
            "#> Joined...\n",
            "[Jul 14, 22:02:47] #> Loading collection...\n",
            "0M \n",
            "[Jul 14, 22:02:50] #> Loading codec...\n",
            "[Jul 14, 22:02:50] Loading decompress_residuals_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n",
            "[Jul 14, 22:02:50] Loading packbits_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n",
            "[Jul 14, 22:02:50] #> Loading IVF...\n",
            "[Jul 14, 22:02:50] #> Loading doclens...\n",
            "100% 1/1 [00:00<00:00, 9510.89it/s]\n",
            "[Jul 14, 22:02:50] #> Loading codes and residuals...\n",
            "100% 1/1 [00:00<00:00, 1647.41it/s]\n",
            "[Jul 14, 22:02:50] #> Loading the queries from data/lm_vectors/colbert/queries.tsv ...\n",
            "[Jul 14, 22:02:50] #> Got 0 queries. All QIDs are unique.\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/HippoRAG/src/colbertv2_knn.py\", line 74, in <module>\n",
            "    nearest_neighbors = retrieve_knn(kb.strings.values, queries.strings.values)\n",
            "  File \"/content/HippoRAG/src/colbertv2_knn.py\", line 45, in retrieve_knn\n",
            "    ranking = searcher.search_all(queries, k=nns)\n",
            "  File \"/usr/local/lib/python3.10/site-packages/colbert/searcher.py\", line 73, in search_all\n",
            "    Q = self.encode(queries_, full_length_search=full_length_search)\n",
            "  File \"/usr/local/lib/python3.10/site-packages/colbert/searcher.py\", line 61, in encode\n",
            "    Q = self.checkpoint.queryFromText(queries, bsize=bsize, to_cpu=True, full_length_search=full_length_search)\n",
            "  File \"/usr/local/lib/python3.10/site-packages/colbert/modeling/checkpoint.py\", line 51, in queryFromText\n",
            "    input_ids, attention_mask = self.query_tokenizer.tensorize(queries, context=context, full_length_search=full_length_search)\n",
            "  File \"/usr/local/lib/python3.10/site-packages/colbert/modeling/tokenization/query_tokenization.py\", line 74, in tensorize\n",
            "    obj = self.tok(batch_text, padding='max_length', truncation=True,\n",
            "  File \"/usr/local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 2803, in __call__\n",
            "    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 2889, in _call_one\n",
            "    return self.batch_encode_plus(\n",
            "  File \"/usr/local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 3080, in batch_encode_plus\n",
            "    return self._batch_encode_plus(\n",
            "  File \"/usr/local/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py\", line 537, in _batch_encode_plus\n",
            "    for key in tokens_and_encodings[0][0].keys():\n",
            "IndexError: list index out of range\n",
            "***Creating graph for the second time...\n",
            "[nltk_data] Downloading package stopwords to /tmp...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "100% 4/4 [00:00<00:00, 6631.31it/s]\n",
            "Correct Wiki Format: 0 out of 4\n",
            "0it [00:00, ?it/s]\n",
            "Creating Graph\n",
            "100% 4/4 [00:00<00:00, 6184.01it/s]\n",
            "Loading Vectors\n",
            "Augmenting Graph from Similarity\n",
            "100% 50/50 [00:00<00:00, 118886.17it/s]\n",
            "Saving Graph\n",
            "                                                      1\n",
            "0                                                      \n",
            "Total Phrases                                       129\n",
            "Unique Phrases                                       50\n",
            "Number of Individual Triples                         43\n",
            "Number of Incorrectly Formatted Triples (ChatGP...    0\n",
            "Number of Triples w/o NER Entities (ChatGPT Error)    7\n",
            "Number of Unique Individual Triples                  43\n",
            "Number of Entities                                   86\n",
            "Number of Relations                                  56\n",
            "Number of Unique Entities                            50\n",
            "Number of Synonymy Edges                             13\n",
            "Number of Unique Relations                           21\n",
            "***Creating colbert indexing files...\n",
            "\n",
            "\n",
            "[Jul 14, 22:02:56] #> Creating directory data/lm_vectors/colbert/inventures2024/corpus/indexes/exp/colbertv2.0 \n",
            "\n",
            "\n",
            "#> Starting...\n",
            "nranks = 1 \t num_gpus = 1 \t device=0\n",
            "{\n",
            "    \"query_token_id\": \"[unused0]\",\n",
            "    \"doc_token_id\": \"[unused1]\",\n",
            "    \"query_token\": \"[Q]\",\n",
            "    \"doc_token\": \"[D]\",\n",
            "    \"ncells\": null,\n",
            "    \"centroid_score_threshold\": null,\n",
            "    \"ndocs\": null,\n",
            "    \"load_index_with_mmap\": false,\n",
            "    \"index_path\": null,\n",
            "    \"index_bsize\": 64,\n",
            "    \"nbits\": 2,\n",
            "    \"kmeans_niters\": 20,\n",
            "    \"resume\": false,\n",
            "    \"similarity\": \"cosine\",\n",
            "    \"bsize\": 64,\n",
            "    \"accumsteps\": 1,\n",
            "    \"lr\": 1e-5,\n",
            "    \"maxsteps\": 400000,\n",
            "    \"save_every\": null,\n",
            "    \"warmup\": 20000,\n",
            "    \"warmup_bert\": null,\n",
            "    \"relu\": false,\n",
            "    \"nway\": 64,\n",
            "    \"use_ib_negatives\": true,\n",
            "    \"reranker\": false,\n",
            "    \"distillation_alpha\": 1.0,\n",
            "    \"ignore_scores\": false,\n",
            "    \"model_name\": null,\n",
            "    \"query_maxlen\": 32,\n",
            "    \"attend_to_mask_tokens\": false,\n",
            "    \"interaction\": \"colbert\",\n",
            "    \"dim\": 128,\n",
            "    \"doc_maxlen\": 180,\n",
            "    \"mask_punctuation\": true,\n",
            "    \"checkpoint\": \"exp\\/colbertv2.0\",\n",
            "    \"triples\": \"\\/future\\/u\\/okhattab\\/root\\/unit\\/experiments\\/2021.10\\/downstream.distillation.round2.2_score\\/round2.nway6.cosine.ib\\/examples.64.json\",\n",
            "    \"collection\": \"data\\/lm_vectors\\/colbert\\/inventures2024_corpus_186.tsv\",\n",
            "    \"queries\": \"\\/future\\/u\\/okhattab\\/data\\/MSMARCO\\/queries.train.tsv\",\n",
            "    \"index_name\": \"exp\\/colbertv2.0\",\n",
            "    \"overwrite\": false,\n",
            "    \"root\": \"data\\/lm_vectors\\/colbert\\/inventures2024\",\n",
            "    \"experiment\": \"corpus\",\n",
            "    \"index_root\": null,\n",
            "    \"name\": \"2024-07\\/14\\/22.02.55\",\n",
            "    \"rank\": 0,\n",
            "    \"nranks\": 1,\n",
            "    \"amp\": true,\n",
            "    \"gpus\": 1,\n",
            "    \"avoid_fork_if_possible\": false\n",
            "}\n",
            "[Jul 14, 22:03:00] #> Loading collection...\n",
            "0M \n",
            "[Jul 14, 22:03:03] [0] \t\t # of sampled PIDs = 186 \t sampled_pids[:3] = [106, 2, 76]\n",
            "[Jul 14, 22:03:03] [0] \t\t #> Encoding 186 passages..\n",
            "[Jul 14, 22:03:04] [0] \t\t avg_doclen_est = 139.22579956054688 \t len(local_sample) = 186\n",
            "[Jul 14, 22:03:04] [0] \t\t Creating 2,048 partitions.\n",
            "[Jul 14, 22:03:04] [0] \t\t *Estimated* 25,895 embeddings.\n",
            "[Jul 14, 22:03:04] [0] \t\t #> Saving the indexing plan to data/lm_vectors/colbert/inventures2024/corpus/indexes/exp/colbertv2.0/plan.json ..\n",
            "WARNING clustering 24602 points to 2048 centroids: please provide at least 79872 training points\n",
            "Clustering 24602 points in 128D to 2048 clusters, redo 1 times, 20 iterations\n",
            "  Preprocessing in 0.00 s\n",
            "  Iteration 19 (0.16 s, search 0.13 s): objective=4465.86 imbalance=1.472 nsplit=0       \n",
            "[Jul 14, 22:03:05] Loading decompress_residuals_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n",
            "[Jul 14, 22:03:05] Loading packbits_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n",
            "[0.03, 0.032, 0.03, 0.029, 0.031, 0.033, 0.028, 0.028, 0.028, 0.034, 0.029, 0.03, 0.031, 0.03, 0.03, 0.029, 0.028, 0.031, 0.03, 0.032, 0.03, 0.031, 0.031, 0.03, 0.031, 0.029, 0.031, 0.03, 0.03, 0.029, 0.03, 0.033, 0.033, 0.03, 0.031, 0.031, 0.031, 0.032, 0.032, 0.034, 0.032, 0.031, 0.031, 0.03, 0.031, 0.03, 0.033, 0.034, 0.029, 0.031, 0.028, 0.031, 0.032, 0.029, 0.031, 0.031, 0.033, 0.03, 0.035, 0.032, 0.029, 0.031, 0.033, 0.032, 0.033, 0.033, 0.03, 0.032, 0.03, 0.029, 0.032, 0.03, 0.031, 0.033, 0.031, 0.03, 0.032, 0.03, 0.031, 0.031, 0.034, 0.032, 0.032, 0.029, 0.032, 0.03, 0.031, 0.032, 0.028, 0.034, 0.031, 0.033, 0.032, 0.033, 0.029, 0.031, 0.032, 0.03, 0.03, 0.03, 0.032, 0.031, 0.028, 0.032, 0.031, 0.029, 0.029, 0.03, 0.032, 0.028, 0.031, 0.03, 0.032, 0.031, 0.028, 0.029, 0.031, 0.032, 0.031, 0.032, 0.029, 0.029, 0.03, 0.032, 0.029, 0.033, 0.032, 0.027]\n",
            "[Jul 14, 22:03:05] #> Got bucket_cutoffs_quantiles = tensor([0.2500, 0.5000, 0.7500], device='cuda:0') and bucket_weights_quantiles = tensor([0.1250, 0.3750, 0.6250, 0.8750], device='cuda:0')\n",
            "[Jul 14, 22:03:05] #> Got bucket_cutoffs = tensor([-0.0217,  0.0000,  0.0216], device='cuda:0') and bucket_weights = tensor([-0.0422, -0.0095,  0.0094,  0.0423], device='cuda:0')\n",
            "[Jul 14, 22:03:05] avg_residual = 0.0308380126953125\n",
            "0it [00:00, ?it/s][Jul 14, 22:03:05] [0] \t\t #> Encoding 186 passages..\n",
            "[Jul 14, 22:03:05] [0] \t\t #> Saving chunk 0: \t 186 passages and 25,896 embeddings. From #0 onward.\n",
            "1it [00:00,  1.62it/s]\n",
            "[Jul 14, 22:03:05] [0] \t\t #> Checking all files were saved...\n",
            "[Jul 14, 22:03:05] [0] \t\t Found all files!\n",
            "[Jul 14, 22:03:05] [0] \t\t #> Building IVF...\n",
            "[Jul 14, 22:03:05] [0] \t\t #> Loading codes...\n",
            "100% 1/1 [00:00<00:00, 2295.73it/s]\n",
            "[Jul 14, 22:03:05] [0] \t\t Sorting codes...\n",
            "[Jul 14, 22:03:05] [0] \t\t Getting unique codes...\n",
            "[Jul 14, 22:03:05] #> Optimizing IVF to store map from centroids to list of pids..\n",
            "[Jul 14, 22:03:05] #> Building the emb2pid mapping..\n",
            "[Jul 14, 22:03:05] len(emb2pid) = 25896\n",
            "100% 2048/2048 [00:00<00:00, 163325.37it/s]\n",
            "[Jul 14, 22:03:05] #> Saved optimized IVF to data/lm_vectors/colbert/inventures2024/corpus/indexes/exp/colbertv2.0/ivf.pid.pt\n",
            "[Jul 14, 22:03:05] [0] \t\t #> Saving the indexing metadata to data/lm_vectors/colbert/inventures2024/corpus/indexes/exp/colbertv2.0/metadata.json ..\n",
            "#> Joined...\n",
            "\n",
            "\n",
            "[Jul 14, 22:03:07] #> Creating directory data/lm_vectors/colbert/inventures2024/phrase/indexes/exp/colbertv2.0 \n",
            "\n",
            "\n",
            "#> Starting...\n",
            "nranks = 1 \t num_gpus = 1 \t device=0\n",
            "{\n",
            "    \"query_token_id\": \"[unused0]\",\n",
            "    \"doc_token_id\": \"[unused1]\",\n",
            "    \"query_token\": \"[Q]\",\n",
            "    \"doc_token\": \"[D]\",\n",
            "    \"ncells\": null,\n",
            "    \"centroid_score_threshold\": null,\n",
            "    \"ndocs\": null,\n",
            "    \"load_index_with_mmap\": false,\n",
            "    \"index_path\": null,\n",
            "    \"index_bsize\": 64,\n",
            "    \"nbits\": 2,\n",
            "    \"kmeans_niters\": 20,\n",
            "    \"resume\": false,\n",
            "    \"similarity\": \"cosine\",\n",
            "    \"bsize\": 64,\n",
            "    \"accumsteps\": 1,\n",
            "    \"lr\": 1e-5,\n",
            "    \"maxsteps\": 400000,\n",
            "    \"save_every\": null,\n",
            "    \"warmup\": 20000,\n",
            "    \"warmup_bert\": null,\n",
            "    \"relu\": false,\n",
            "    \"nway\": 64,\n",
            "    \"use_ib_negatives\": true,\n",
            "    \"reranker\": false,\n",
            "    \"distillation_alpha\": 1.0,\n",
            "    \"ignore_scores\": false,\n",
            "    \"model_name\": null,\n",
            "    \"query_maxlen\": 32,\n",
            "    \"attend_to_mask_tokens\": false,\n",
            "    \"interaction\": \"colbert\",\n",
            "    \"dim\": 128,\n",
            "    \"doc_maxlen\": 180,\n",
            "    \"mask_punctuation\": true,\n",
            "    \"checkpoint\": \"exp\\/colbertv2.0\",\n",
            "    \"triples\": \"\\/future\\/u\\/okhattab\\/root\\/unit\\/experiments\\/2021.10\\/downstream.distillation.round2.2_score\\/round2.nway6.cosine.ib\\/examples.64.json\",\n",
            "    \"collection\": \"data\\/lm_vectors\\/colbert\\/inventures2024_phrase_50.tsv\",\n",
            "    \"queries\": \"\\/future\\/u\\/okhattab\\/data\\/MSMARCO\\/queries.train.tsv\",\n",
            "    \"index_name\": \"exp\\/colbertv2.0\",\n",
            "    \"overwrite\": false,\n",
            "    \"root\": \"data\\/lm_vectors\\/colbert\\/inventures2024\",\n",
            "    \"experiment\": \"phrase\",\n",
            "    \"index_root\": null,\n",
            "    \"name\": \"2024-07\\/14\\/22.02.55\",\n",
            "    \"rank\": 0,\n",
            "    \"nranks\": 1,\n",
            "    \"amp\": true,\n",
            "    \"gpus\": 1,\n",
            "    \"avoid_fork_if_possible\": false\n",
            "}\n",
            "[Jul 14, 22:03:11] #> Loading collection...\n",
            "0M \n",
            "[Jul 14, 22:03:13] [0] \t\t # of sampled PIDs = 50 \t sampled_pids[:3] = [26, 46, 0]\n",
            "[Jul 14, 22:03:13] [0] \t\t #> Encoding 50 passages..\n",
            "[Jul 14, 22:03:14] [0] \t\t avg_doclen_est = 6.960000038146973 \t len(local_sample) = 50\n",
            "[Jul 14, 22:03:14] [0] \t\t Creating 256 partitions.\n",
            "[Jul 14, 22:03:14] [0] \t\t *Estimated* 348 embeddings.\n",
            "[Jul 14, 22:03:14] [0] \t\t #> Saving the indexing plan to data/lm_vectors/colbert/inventures2024/phrase/indexes/exp/colbertv2.0/plan.json ..\n",
            "WARNING clustering 331 points to 256 centroids: please provide at least 9984 training points\n",
            "Clustering 331 points in 128D to 256 clusters, redo 1 times, 20 iterations\n",
            "  Preprocessing in 0.00 s\n",
            "  Iteration 19 (0.01 s, search 0.00 s): objective=14.6024 imbalance=1.180 nsplit=0       \n",
            "[Jul 14, 22:03:14] Loading decompress_residuals_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n",
            "[Jul 14, 22:03:14] Loading packbits_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n",
            "[0.048, 0.032, 0.037, 0.031, 0.04, 0.036, 0.04, 0.029, 0.055, 0.037, 0.037, 0.041, 0.041, 0.025, 0.032, 0.03, 0.032, 0.029, 0.03, 0.042, 0.053, 0.048, 0.051, 0.052, 0.032, 0.028, 0.024, 0.029, 0.032, 0.034, 0.037, 0.036, 0.044, 0.046, 0.039, 0.033, 0.031, 0.039, 0.039, 0.038, 0.03, 0.039, 0.03, 0.045, 0.037, 0.034, 0.027, 0.058, 0.033, 0.036, 0.048, 0.034, 0.035, 0.026, 0.034, 0.037, 0.041, 0.032, 0.042, 0.036, 0.035, 0.039, 0.03, 0.045, 0.043, 0.034, 0.041, 0.027, 0.026, 0.032, 0.031, 0.038, 0.044, 0.033, 0.031, 0.048, 0.032, 0.033, 0.032, 0.053, 0.028, 0.037, 0.048, 0.025, 0.024, 0.03, 0.042, 0.031, 0.035, 0.043, 0.026, 0.036, 0.031, 0.027, 0.044, 0.035, 0.045, 0.034, 0.044, 0.032, 0.037, 0.037, 0.036, 0.033, 0.035, 0.04, 0.038, 0.036, 0.037, 0.024, 0.032, 0.041, 0.047, 0.028, 0.037, 0.032, 0.038, 0.029, 0.031, 0.032, 0.029, 0.04, 0.043, 0.037, 0.044, 0.052, 0.035, 0.038]\n",
            "[Jul 14, 22:03:15] #> Got bucket_cutoffs_quantiles = tensor([0.2500, 0.5000, 0.7500], device='cuda:0') and bucket_weights_quantiles = tensor([0.1250, 0.3750, 0.6250, 0.8750], device='cuda:0')\n",
            "[Jul 14, 22:03:15] #> Got bucket_cutoffs = tensor([-0.0269, -0.0004,  0.0264], device='cuda:0') and bucket_weights = tensor([-0.0494, -0.0111,  0.0110,  0.0491], device='cuda:0')\n",
            "[Jul 14, 22:03:15] avg_residual = 0.036407470703125\n",
            "0it [00:00, ?it/s][Jul 14, 22:03:15] [0] \t\t #> Encoding 50 passages..\n",
            "[Jul 14, 22:03:15] [0] \t\t #> Saving chunk 0: \t 50 passages and 348 embeddings. From #0 onward.\n",
            "1it [00:00, 21.78it/s]\n",
            "[Jul 14, 22:03:15] [0] \t\t #> Checking all files were saved...\n",
            "[Jul 14, 22:03:15] [0] \t\t Found all files!\n",
            "[Jul 14, 22:03:15] [0] \t\t #> Building IVF...\n",
            "[Jul 14, 22:03:15] [0] \t\t #> Loading codes...\n",
            "100% 1/1 [00:00<00:00, 3218.96it/s]\n",
            "[Jul 14, 22:03:15] [0] \t\t Sorting codes...\n",
            "[Jul 14, 22:03:15] [0] \t\t Getting unique codes...\n",
            "[Jul 14, 22:03:15] #> Optimizing IVF to store map from centroids to list of pids..\n",
            "[Jul 14, 22:03:15] #> Building the emb2pid mapping..\n",
            "[Jul 14, 22:03:15] len(emb2pid) = 348\n",
            "100% 256/256 [00:00<00:00, 179285.66it/s]\n",
            "[Jul 14, 22:03:15] #> Saved optimized IVF to data/lm_vectors/colbert/inventures2024/phrase/indexes/exp/colbertv2.0/ivf.pid.pt\n",
            "[Jul 14, 22:03:15] [0] \t\t #> Saving the indexing metadata to data/lm_vectors/colbert/inventures2024/phrase/indexes/exp/colbertv2.0/metadata.json ..\n",
            "#> Joined...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5uOuYiGB1Wje",
        "outputId": "5824ac5e-2a9e-45b7-cb74-a4c159ad45eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hi\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create the test program\n",
        "%%shell\n",
        "cat > /content/HippoRAG/src/hippo_test.py<< EOF\n",
        "\n",
        "import sys\n",
        "sys.path.insert(0,'/content/HippoRAG')\n",
        "import argparse\n",
        "from langchain_util import LangChainModel\n",
        "from qa.qa_reader import qa_read\n",
        "from hipporag import HippoRAG\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--dataset', type=str)\n",
        "    parser.add_argument('--query', type=str)\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    hipporag = HippoRAG(corpus_name=args.dataset, extraction_model='openai', extraction_model_name='gpt-3.5-turbo-1106',\n",
        "                 graph_creating_retriever_name='colbertv2', qa_model=LangChainModel('openai', 'gpt-3.5-turbo'))\n",
        "\n",
        "    qa_few_shot_samples = None\n",
        "    queries = [args.query]\n",
        "    for query in queries:\n",
        "        ranks, scores, logs = hipporag.rank_docs(query, top_k=2)\n",
        "        retrieved_passages = [hipporag.get_passage_by_idx(rank) for rank in ranks]\n",
        "        response = qa_read(query, retrieved_passages, qa_few_shot_samples, hipporag.qa_model)\n",
        "        print(f\"{response=}\")\n",
        "        print(ranks)\n",
        "        print(scores)\n",
        "        print(logs)\n",
        "EOF"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4nBY_8ynklam",
        "outputId": "284a7ebd-cf9a-47d8-a6d4-e6acc296f64b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd /content/HippoRAG; python src/hippo_test.py --dataset $data --query \"What is Macleod Foyer?\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GkHUEJNhTp9H",
        "outputId": "d83585ee-a452-4158-d439-7c987f2e37e3"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rBuilding Graph:   0% 0/99 [00:00<?, ?it/s]\rBuilding Graph: 100% 99/99 [00:00<00:00, 314811.29it/s]\n",
            "2024-07-14 22:03:34,628 - hipporag - INFO - Graph built: num vertices: 50, num_edges: 99\n",
            "\n",
            "\n",
            "[Jul 14, 22:03:34] #> Creating directory data/lm_vectors/colbert/inventures2024/phrase/indexes/nbits_2 \n",
            "\n",
            "\n",
            "#> Starting...\n",
            "nranks = 1 \t num_gpus = 1 \t device=0\n",
            "{\n",
            "    \"query_token_id\": \"[unused0]\",\n",
            "    \"doc_token_id\": \"[unused1]\",\n",
            "    \"query_token\": \"[Q]\",\n",
            "    \"doc_token\": \"[D]\",\n",
            "    \"ncells\": null,\n",
            "    \"centroid_score_threshold\": null,\n",
            "    \"ndocs\": null,\n",
            "    \"load_index_with_mmap\": false,\n",
            "    \"index_path\": null,\n",
            "    \"index_bsize\": 64,\n",
            "    \"nbits\": 2,\n",
            "    \"kmeans_niters\": 20,\n",
            "    \"resume\": false,\n",
            "    \"similarity\": \"cosine\",\n",
            "    \"bsize\": 64,\n",
            "    \"accumsteps\": 1,\n",
            "    \"lr\": 1e-5,\n",
            "    \"maxsteps\": 400000,\n",
            "    \"save_every\": null,\n",
            "    \"warmup\": 20000,\n",
            "    \"warmup_bert\": null,\n",
            "    \"relu\": false,\n",
            "    \"nway\": 64,\n",
            "    \"use_ib_negatives\": true,\n",
            "    \"reranker\": false,\n",
            "    \"distillation_alpha\": 1.0,\n",
            "    \"ignore_scores\": false,\n",
            "    \"model_name\": null,\n",
            "    \"query_maxlen\": 32,\n",
            "    \"attend_to_mask_tokens\": false,\n",
            "    \"interaction\": \"colbert\",\n",
            "    \"dim\": 128,\n",
            "    \"doc_maxlen\": 180,\n",
            "    \"mask_punctuation\": true,\n",
            "    \"checkpoint\": \"exp\\/colbertv2.0\",\n",
            "    \"triples\": \"\\/future\\/u\\/okhattab\\/root\\/unit\\/experiments\\/2021.10\\/downstream.distillation.round2.2_score\\/round2.nway6.cosine.ib\\/examples.64.json\",\n",
            "    \"collection\": \"data\\/lm_vectors\\/colbert\\/inventures2024_phrase_50.tsv\",\n",
            "    \"queries\": \"\\/future\\/u\\/okhattab\\/data\\/MSMARCO\\/queries.train.tsv\",\n",
            "    \"index_name\": \"nbits_2\",\n",
            "    \"overwrite\": false,\n",
            "    \"root\": \"data\\/lm_vectors\\/colbert\\/inventures2024\",\n",
            "    \"experiment\": \"phrase\",\n",
            "    \"index_root\": null,\n",
            "    \"name\": \"2024-07\\/14\\/22.03.33\",\n",
            "    \"rank\": 0,\n",
            "    \"nranks\": 1,\n",
            "    \"amp\": true,\n",
            "    \"gpus\": 1,\n",
            "    \"avoid_fork_if_possible\": false\n",
            "}\n",
            "[Jul 14, 22:03:45] #> Loading collection...\n",
            "0M \n",
            "[Jul 14, 22:03:46] [0] \t\t # of sampled PIDs = 50 \t sampled_pids[:3] = [26, 46, 0]\n",
            "[Jul 14, 22:03:46] [0] \t\t #> Encoding 50 passages..\n",
            "[Jul 14, 22:03:47] [0] \t\t avg_doclen_est = 6.960000038146973 \t len(local_sample) = 50\n",
            "[Jul 14, 22:03:47] [0] \t\t Creating 256 partitions.\n",
            "[Jul 14, 22:03:47] [0] \t\t *Estimated* 348 embeddings.\n",
            "[Jul 14, 22:03:47] [0] \t\t #> Saving the indexing plan to data/lm_vectors/colbert/inventures2024/phrase/indexes/nbits_2/plan.json ..\n",
            "WARNING clustering 331 points to 256 centroids: please provide at least 9984 training points\n",
            "Clustering 331 points in 128D to 256 clusters, redo 1 times, 20 iterations\n",
            "  Preprocessing in 0.00 s\n",
            "  Iteration 19 (0.01 s, search 0.00 s): objective=14.6024 imbalance=1.180 nsplit=0       \n",
            "[Jul 14, 22:03:47] Loading decompress_residuals_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n",
            "[Jul 14, 22:03:48] Loading packbits_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n",
            "[0.048, 0.032, 0.037, 0.031, 0.04, 0.036, 0.04, 0.029, 0.055, 0.037, 0.037, 0.041, 0.041, 0.025, 0.032, 0.03, 0.032, 0.029, 0.03, 0.042, 0.053, 0.048, 0.051, 0.052, 0.032, 0.028, 0.024, 0.029, 0.032, 0.034, 0.037, 0.036, 0.044, 0.046, 0.039, 0.033, 0.031, 0.039, 0.039, 0.038, 0.03, 0.039, 0.03, 0.045, 0.037, 0.034, 0.027, 0.058, 0.033, 0.036, 0.048, 0.034, 0.035, 0.026, 0.034, 0.037, 0.041, 0.032, 0.042, 0.036, 0.035, 0.039, 0.03, 0.045, 0.043, 0.034, 0.041, 0.027, 0.026, 0.032, 0.031, 0.038, 0.044, 0.033, 0.031, 0.048, 0.032, 0.033, 0.032, 0.053, 0.028, 0.037, 0.048, 0.025, 0.024, 0.03, 0.042, 0.031, 0.035, 0.043, 0.026, 0.036, 0.031, 0.027, 0.044, 0.035, 0.045, 0.034, 0.044, 0.032, 0.037, 0.037, 0.036, 0.033, 0.035, 0.04, 0.038, 0.036, 0.037, 0.024, 0.032, 0.041, 0.047, 0.028, 0.037, 0.032, 0.038, 0.029, 0.031, 0.032, 0.029, 0.04, 0.043, 0.037, 0.044, 0.052, 0.035, 0.038]\n",
            "[Jul 14, 22:03:48] #> Got bucket_cutoffs_quantiles = tensor([0.2500, 0.5000, 0.7500], device='cuda:0') and bucket_weights_quantiles = tensor([0.1250, 0.3750, 0.6250, 0.8750], device='cuda:0')\n",
            "[Jul 14, 22:03:48] #> Got bucket_cutoffs = tensor([-0.0269, -0.0004,  0.0264], device='cuda:0') and bucket_weights = tensor([-0.0494, -0.0111,  0.0110,  0.0491], device='cuda:0')\n",
            "[Jul 14, 22:03:48] avg_residual = 0.036407470703125\n",
            "0it [00:00, ?it/s][Jul 14, 22:03:48] [0] \t\t #> Encoding 50 passages..\n",
            "[Jul 14, 22:03:48] [0] \t\t #> Saving chunk 0: \t 50 passages and 348 embeddings. From #0 onward.\n",
            "1it [00:00, 23.50it/s]\n",
            "[Jul 14, 22:03:48] [0] \t\t #> Checking all files were saved...\n",
            "[Jul 14, 22:03:48] [0] \t\t Found all files!\n",
            "[Jul 14, 22:03:48] [0] \t\t #> Building IVF...\n",
            "[Jul 14, 22:03:48] [0] \t\t #> Loading codes...\n",
            "100% 1/1 [00:00<00:00, 2798.07it/s]\n",
            "[Jul 14, 22:03:48] [0] \t\t Sorting codes...\n",
            "[Jul 14, 22:03:48] [0] \t\t Getting unique codes...\n",
            "[Jul 14, 22:03:48] #> Optimizing IVF to store map from centroids to list of pids..\n",
            "[Jul 14, 22:03:48] #> Building the emb2pid mapping..\n",
            "[Jul 14, 22:03:48] len(emb2pid) = 348\n",
            "100% 256/256 [00:00<00:00, 74081.81it/s]\n",
            "[Jul 14, 22:03:48] #> Saved optimized IVF to data/lm_vectors/colbert/inventures2024/phrase/indexes/nbits_2/ivf.pid.pt\n",
            "[Jul 14, 22:03:48] [0] \t\t #> Saving the indexing metadata to data/lm_vectors/colbert/inventures2024/phrase/indexes/nbits_2/metadata.json ..\n",
            "#> Joined...\n",
            "[Jul 14, 22:03:49] #> Loading collection...\n",
            "0M \n",
            "[Jul 14, 22:03:52] #> Loading codec...\n",
            "[Jul 14, 22:03:52] Loading decompress_residuals_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n",
            "[Jul 14, 22:03:52] Loading packbits_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n",
            "[Jul 14, 22:03:52] #> Loading IVF...\n",
            "[Jul 14, 22:03:52] #> Loading doclens...\n",
            "100% 1/1 [00:00<00:00, 8128.50it/s]\n",
            "[Jul 14, 22:03:52] #> Loading codes and residuals...\n",
            "100% 1/1 [00:00<00:00, 1294.54it/s]\n",
            "2024-07-14 22:03:53,513 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "1it [00:00, 98.09it/s]\n",
            "pagerank chunk: 100% 1/1 [00:00<00:00, 208.51it/s]\n",
            "2024-07-14 22:03:55,772 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "response='Thought: The text mentions Macleod Foyer as one of the locations where refreshments will be available during the Networking Break session at Inventures. From the context provided, it seems that Macleod Foyer is a specific area within the conference center where attendees can grab a drink and a snack during the event. It is likely a designated space for networking and refreshments during the session.\\n\\nAnswer: Macleod Foyer appears to be a designated area within the conference center at Inventures where attendees can access refreshments during the Networking Break session.'\n",
            "[3, 1]\n",
            "[1.0, 0.001358790775319467]\n",
            "{'named_entities': ['macleod foyer'], 'linked_node_scores': [['macleod foyer', 'macleod foyer', 1.0]], '1-hop_graph_for_linked_nodes': [['networking break', 1.0], ['networking break', 1.0, 'inv']], 'top_ranked_nodes': ['macleod foyer', 'networking break', 'session 3', 'inventures', 'exhibition hall foyer', 'telus foyer', 'glen foyer', 'session 1', 'networking', 'session 0', 'online registration system', 'session 2', 'badge collection is available in the exhibition hall foyer area of the convention centre', 'inventures 2024', 'breakfast bites   networking', 'breakfast   networking', 'join us for coffee  light breakfast bites  and networking before the start of inventures programming', 'online registration link', 'hall e', 'registration'], 'nodes_in_retrieved_doc': [['exhibition hall foyer', 'glen foyer', 'inventures', 'macleod foyer', 'networking', 'networking break', 'session 3', 'telus foyer'], ['breakfast   networking', 'breakfast bites   networking', 'exhibition hall foyer', 'hall e', 'inventures', 'join us for coffee  light breakfast bites  and networking before the start of inventures programming', 'session 1'], ['badge', 'badge collection for students and media is available at the kiosk window counter just inside the ground level entrance to the convention centre north building', 'badge collection is available in the exhibition hall foyer area of the convention centre', 'inventures', 'inventures 2024', 'kiosk window counter', 'media accreditation', 'north building', 'online registration link', 'online registration system', 'registration', 'session 0'], ['academia', 'advertising', 'award winning marketer', 'chief executive officer at alberta innovates', 'chief lee crowchild', 'cultural translator', 'dancers', 'darrell brertton', 'dr  marcus collins', 'expertise', 'former xakiji  chief  at tsuut ina first nation', 'humanity and technology', 'insights', 'inventures', 'keynote', 'laura kilcrease', 'leverage culture to drive action where humanity meets technology', 'local and national musicians', 'music', 'nexus of humanity and technology', 'opening ceremony and keynote  where humanity meets technology', 'opening keynote', 'saddle lake cree nation', 'session 2', 'singers', 'sport', 'technology']]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !cd /content/HippoRAG; python src/hippo_test.py --dataset $data --query \"Who directed the movie Black Hawk Down?\"\n",
        "\n",
        "!cd /content/HippoRAG; python src/hippo_test.py --dataset $data --query \"Who is Laura Kilcrease?\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fMNmmDrMCYTe",
        "outputId": "669e38ba-5197-441e-9a25-4bde356350d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rBuilding Graph:   0% 0/82 [00:00<?, ?it/s]\rBuilding Graph: 100% 82/82 [00:00<00:00, 320533.95it/s]\n",
            "2024-07-13 18:13:34,532 - hipporag - INFO - Graph built: num vertices: 43, num_edges: 82\n",
            "\n",
            "\n",
            "[Jul 13, 18:13:34] #> Creating directory data/lm_vectors/colbert/inventures2024/phrase/indexes/nbits_2 \n",
            "\n",
            "\n",
            "#> Starting...\n",
            "nranks = 1 \t num_gpus = 1 \t device=0\n",
            "{\n",
            "    \"query_token_id\": \"[unused0]\",\n",
            "    \"doc_token_id\": \"[unused1]\",\n",
            "    \"query_token\": \"[Q]\",\n",
            "    \"doc_token\": \"[D]\",\n",
            "    \"ncells\": null,\n",
            "    \"centroid_score_threshold\": null,\n",
            "    \"ndocs\": null,\n",
            "    \"load_index_with_mmap\": false,\n",
            "    \"index_path\": null,\n",
            "    \"index_bsize\": 64,\n",
            "    \"nbits\": 2,\n",
            "    \"kmeans_niters\": 20,\n",
            "    \"resume\": false,\n",
            "    \"similarity\": \"cosine\",\n",
            "    \"bsize\": 64,\n",
            "    \"accumsteps\": 1,\n",
            "    \"lr\": 1e-5,\n",
            "    \"maxsteps\": 400000,\n",
            "    \"save_every\": null,\n",
            "    \"warmup\": 20000,\n",
            "    \"warmup_bert\": null,\n",
            "    \"relu\": false,\n",
            "    \"nway\": 64,\n",
            "    \"use_ib_negatives\": true,\n",
            "    \"reranker\": false,\n",
            "    \"distillation_alpha\": 1.0,\n",
            "    \"ignore_scores\": false,\n",
            "    \"model_name\": null,\n",
            "    \"query_maxlen\": 32,\n",
            "    \"attend_to_mask_tokens\": false,\n",
            "    \"interaction\": \"colbert\",\n",
            "    \"dim\": 128,\n",
            "    \"doc_maxlen\": 180,\n",
            "    \"mask_punctuation\": true,\n",
            "    \"checkpoint\": \"exp\\/colbertv2.0\",\n",
            "    \"triples\": \"\\/future\\/u\\/okhattab\\/root\\/unit\\/experiments\\/2021.10\\/downstream.distillation.round2.2_score\\/round2.nway6.cosine.ib\\/examples.64.json\",\n",
            "    \"collection\": \"data\\/lm_vectors\\/colbert\\/inventures2024_phrase_43.tsv\",\n",
            "    \"queries\": \"\\/future\\/u\\/okhattab\\/data\\/MSMARCO\\/queries.train.tsv\",\n",
            "    \"index_name\": \"nbits_2\",\n",
            "    \"overwrite\": false,\n",
            "    \"root\": \"data\\/lm_vectors\\/colbert\\/inventures2024\",\n",
            "    \"experiment\": \"phrase\",\n",
            "    \"index_root\": null,\n",
            "    \"name\": \"2024-07\\/13\\/18.13.33\",\n",
            "    \"rank\": 0,\n",
            "    \"nranks\": 1,\n",
            "    \"amp\": true,\n",
            "    \"gpus\": 1,\n",
            "    \"avoid_fork_if_possible\": false\n",
            "}\n",
            "[Jul 13, 18:13:43] #> Loading collection...\n",
            "0M \n",
            "[Jul 13, 18:13:46] [0] \t\t # of sampled PIDs = 43 \t sampled_pids[:3] = [26, 0, 19]\n",
            "[Jul 13, 18:13:46] [0] \t\t #> Encoding 43 passages..\n",
            "[Jul 13, 18:13:46] [0] \t\t avg_doclen_est = 7.348837375640869 \t len(local_sample) = 43\n",
            "[Jul 13, 18:13:46] [0] \t\t Creating 256 partitions.\n",
            "[Jul 13, 18:13:46] [0] \t\t *Estimated* 316 embeddings.\n",
            "[Jul 13, 18:13:46] [0] \t\t #> Saving the indexing plan to data/lm_vectors/colbert/inventures2024/phrase/indexes/nbits_2/plan.json ..\n",
            "WARNING clustering 301 points to 256 centroids: please provide at least 9984 training points\n",
            "Clustering 301 points in 128D to 256 clusters, redo 1 times, 20 iterations\n",
            "  Preprocessing in 0.00 s\n",
            "  Iteration 19 (0.01 s, search 0.00 s): objective=5.62122 imbalance=1.139 nsplit=0       \n",
            "[Jul 13, 18:13:47] Loading decompress_residuals_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n",
            "[Jul 13, 18:13:47] Loading packbits_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n",
            "[0.03, 0.032, 0.025, 0.034, 0.042, 0.029, 0.025, 0.036, 0.046, 0.028, 0.03, 0.035, 0.039, 0.044, 0.029, 0.03, 0.026, 0.034, 0.029, 0.033, 0.024, 0.044, 0.034, 0.027, 0.026, 0.021, 0.041, 0.033, 0.033, 0.038, 0.021, 0.035, 0.036, 0.039, 0.038, 0.023, 0.044, 0.026, 0.032, 0.029, 0.033, 0.038, 0.029, 0.04, 0.029, 0.052, 0.051, 0.037, 0.033, 0.022, 0.032, 0.027, 0.022, 0.037, 0.024, 0.044, 0.041, 0.032, 0.037, 0.023, 0.031, 0.031, 0.033, 0.038, 0.067, 0.029, 0.039, 0.033, 0.036, 0.036, 0.028, 0.031, 0.025, 0.028, 0.039, 0.031, 0.028, 0.031, 0.034, 0.032, 0.037, 0.035, 0.034, 0.052, 0.036, 0.036, 0.036, 0.045, 0.038, 0.031, 0.032, 0.045, 0.02, 0.03, 0.027, 0.037, 0.03, 0.024, 0.035, 0.035, 0.033, 0.037, 0.027, 0.034, 0.036, 0.024, 0.027, 0.034, 0.035, 0.027, 0.036, 0.025, 0.047, 0.032, 0.038, 0.019, 0.028, 0.024, 0.034, 0.036, 0.039, 0.039, 0.034, 0.039, 0.032, 0.056, 0.041, 0.029]\n",
            "[Jul 13, 18:13:48] #> Got bucket_cutoffs_quantiles = tensor([0.2500, 0.5000, 0.7500], device='cuda:0') and bucket_weights_quantiles = tensor([0.1250, 0.3750, 0.6250, 0.8750], device='cuda:0')\n",
            "[Jul 13, 18:13:48] #> Got bucket_cutoffs = tensor([-0.0276, -0.0016,  0.0255], device='cuda:0') and bucket_weights = tensor([-0.0484, -0.0132,  0.0113,  0.0471], device='cuda:0')\n",
            "[Jul 13, 18:13:48] avg_residual = 0.033538818359375\n",
            "0it [00:00, ?it/s][Jul 13, 18:13:48] [0] \t\t #> Encoding 43 passages..\n",
            "[Jul 13, 18:13:48] [0] \t\t #> Saving chunk 0: \t 43 passages and 316 embeddings. From #0 onward.\n",
            "1it [00:00, 23.22it/s]\n",
            "[Jul 13, 18:13:48] [0] \t\t #> Checking all files were saved...\n",
            "[Jul 13, 18:13:48] [0] \t\t Found all files!\n",
            "[Jul 13, 18:13:48] [0] \t\t #> Building IVF...\n",
            "[Jul 13, 18:13:48] [0] \t\t #> Loading codes...\n",
            "100% 1/1 [00:00<00:00, 2739.58it/s]\n",
            "[Jul 13, 18:13:48] [0] \t\t Sorting codes...\n",
            "[Jul 13, 18:13:48] [0] \t\t Getting unique codes...\n",
            "[Jul 13, 18:13:48] #> Optimizing IVF to store map from centroids to list of pids..\n",
            "[Jul 13, 18:13:48] #> Building the emb2pid mapping..\n",
            "[Jul 13, 18:13:48] len(emb2pid) = 316\n",
            "100% 256/256 [00:00<00:00, 176922.36it/s]\n",
            "[Jul 13, 18:13:48] #> Saved optimized IVF to data/lm_vectors/colbert/inventures2024/phrase/indexes/nbits_2/ivf.pid.pt\n",
            "[Jul 13, 18:13:48] [0] \t\t #> Saving the indexing metadata to data/lm_vectors/colbert/inventures2024/phrase/indexes/nbits_2/metadata.json ..\n",
            "#> Joined...\n",
            "[Jul 13, 18:13:49] #> Loading collection...\n",
            "0M \n",
            "[Jul 13, 18:13:51] #> Loading codec...\n",
            "[Jul 13, 18:13:51] Loading decompress_residuals_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n",
            "[Jul 13, 18:13:51] Loading packbits_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n",
            "[Jul 13, 18:13:51] #> Loading IVF...\n",
            "[Jul 13, 18:13:51] #> Loading doclens...\n",
            "100% 1/1 [00:00<00:00, 10205.12it/s]\n",
            "[Jul 13, 18:13:51] #> Loading codes and residuals...\n",
            "100% 1/1 [00:00<00:00, 1691.93it/s]\n",
            "2024-07-13 18:13:52,491 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "1it [00:00, 92.78it/s]\n",
            "pagerank chunk: 100% 1/1 [00:00<00:00, 154.67it/s]\n",
            "2024-07-13 18:13:54,323 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "response='Thought: Laura Kilcrease is the Chief Executive Officer at Alberta Innovates, as mentioned in the text. She is one of the speakers at the session titled \"Opening Ceremony and Keynote: Where Humanity Meets Technology\" held at Inventures. \\n\\nAnswer: Laura Kilcrease is the Chief Executive Officer at Alberta Innovates.'\n",
            "[2, 3]\n",
            "[1.0, 0.0]\n",
            "{'named_entities': ['laura kilcrease'], 'linked_node_scores': [['laura kilcrease', 'laura kilcrease', 1.0]], '1-hop_graph_for_linked_nodes': [['chief executive officer at alberta innovates', 1.0], ['chief executive officer at alberta innovates', 1.0, 'inv']], 'top_ranked_nodes': ['laura kilcrease', 'chief executive officer at alberta innovates', 'telus foyer', 'singers', 'session 3', 'session 2', 'session 1', 'session 0', 'saddle lake cree nation', 'registration', 'professor', 'opening ceremony and keynote  where humanity meets technology', 'online registration system', 'online registration link', 'north building', 'nexus of humanity and technology', 'networking break', 'networking', 'media accreditation', 'macleod foyer'], 'nodes_in_retrieved_doc': [['award winning marketer', 'chief executive officer at alberta innovates', 'chief lee crowchild', 'creating culturally contagious ideas', 'cultural translator', 'dancers', 'darrell brertton', 'dr  marcus collins', 'former xakiji  chief  at tsuut ina first nation', 'humanity and technology', 'inventures', 'keynote', 'laura kilcrease', 'local and national musicians', 'nexus of humanity and technology', 'opening ceremony and keynote  where humanity meets technology', 'professor', 'saddle lake cree nation', 'session 2', 'singers'], ['exhibition hall foyer', 'glen foyer', 'inventures', 'macleod foyer', 'networking', 'networking break', 'session 3', 'telus foyer'], ['breakfast   networking', 'breakfast bites   networking', 'exhibition hall foyer', 'hall e', 'inventures', 'join us for coffee  light breakfast bites  and networking before the start of inventures programming', 'session 1'], ['badge', 'badge collection for students and media is available at the kiosk window counter just inside the ground level entrance to the convention centre north building', 'badge collection is available in the exhibition hall foyer area of the convention centre', 'inventures', 'inventures 2024', 'kiosk window counter', 'media accreditation', 'north building', 'online registration link', 'online registration system', 'registration', 'session 0']]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/HippoRAG"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6_adEAKSArTT",
        "outputId": "dbb22eb6-2f51-4cc4-c865-d688f8fd71ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/HippoRAG\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7-PA7YQ-A_tY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Will use the corpus \"case_study_movies\" in the data folder with only 3 passages and Contriever\n",
        "\n",
        "# %env DATA=case_study_movies\n",
        "# %env HF_RETRIEVER=facebook/contriever\n",
        "# %env LLM_MODEL=gpt-3.5-turbo-1106\n",
        "# %env SYNONYM_THRESH=0.8\n",
        "# # GPUS=0,1,2,3\n",
        "# %env GPUS=0\n",
        "# # LLM API provider e.g., 'openai', 'together', see 'src/langchain_util.py'\n",
        "# %env LLM_API=openai\n",
        "# %env extraction_type=ner\n",
        "# # 'all' or 'the number of passages to use'\n",
        "# %env num_passages=3\n",
        "\n",
        "# # bash src/setup_hipporag.sh $DATA $HF_RETRIEVER $LLM $GPUS $SYNONYM_THRESH $LLM_API\n",
        "# # here are the commands inside setup_hipporag.sh, will run here to pass num_passages to 3\n",
        "\n",
        "# # Running Open Information Extraction\n",
        "# !cd /content/HippoRAG; python src/openie_with_retrieval_option_parallel.py --dataset $DATA --llm $LLM_API --model_name $LLM_MODEL --run_ner --num_passages $num_passages # NER and OpenIE for passages\n",
        "# !cd /content/HippoRAG; python src/named_entity_extraction_parallel.py --dataset $DATA --llm $LLM_API --model_name $LLM_MODEL  # NER for queries\n",
        "\n",
        "# # Creating Contriever Graph\n",
        "# !cd /content/HippoRAG; python src/create_graph.py --dataset $DATA --model_name $HF_RETRIEVER --extraction_model $LLM_MODEL --threshold $SYNONYM_THRESH --extraction_type $extraction_type --cosine_sim_edges\n",
        "\n",
        "# # Getting Nearest Neighbor Files\n",
        "# %env CUDA_VISIBLE_DEVICES=0\n",
        "# !cd /content/HippoRAG; python src/RetrievalModule.py --retriever_name $HF_RETRIEVER --string_filename output/query_to_kb.tsv\n",
        "# !cd /content/HippoRAG; python src/RetrievalModule.py --retriever_name $HF_RETRIEVER --string_filename output/kb_to_kb.tsv\n",
        "# !cd /content/HippoRAG; python src/RetrievalModule.py --retriever_name $HF_RETRIEVER --string_filename output/rel_kb_to_kb.tsv\n",
        "\n",
        "# !cd /content/HippoRAG; python src/create_graph.py --dataset $DATA --model_name $HF_RETRIEVER --extraction_model $LLM_MODEL --threshold $SYNONYM_THRESH --create_graph --extraction_type $extraction_type --cosine_sim_edges\n"
      ],
      "metadata": {
        "id": "UNm64wrrt73h"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}